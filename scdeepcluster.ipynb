{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch_geometric as tg\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import csv\n",
    "from node2vec import Node2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 data_path, \n",
    "                 k=5, \n",
    "                 gene_embedding_dim=64, \n",
    "                 string_file=\"string_interactions.tsv\", \n",
    "                 string_query=False, \n",
    "                 output_hvg=\"highly_variable_genes.txt\"):\n",
    "        \"\"\"\n",
    "        Initialize the data processor with the given parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : str\n",
    "            Name of the dataset (e.g., 'Biase', 'Darmanis', etc.)\n",
    "        data_path : str\n",
    "            Path to the input data file.\n",
    "        k : int\n",
    "            Number of nearest neighbors for adjacency construction.\n",
    "        gene_embedding_dim : int\n",
    "            Dimension of the gene embeddings (e.g., 64).\n",
    "        string_file : str\n",
    "            Path to the STRING interactions file (TSV).\n",
    "        string_query : bool\n",
    "            If True, attempt to query the STRING database for interactions.\n",
    "        output_hvg : str\n",
    "            File to write HVGs.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.data_path = data_path\n",
    "        self.k = k\n",
    "        self.gene_embedding_dim = gene_embedding_dim\n",
    "        self.string_file = string_file\n",
    "        self.string_query = string_query\n",
    "        self.output_hvg = output_hvg\n",
    "\n",
    "    def load_dataset(self, dataset_name, data_path):\n",
    "        \"\"\"\n",
    "        Load the dataset specified by dataset_name.\n",
    "        Implement your own logic here depending on how your datasets are stored.\n",
    "        \"\"\"\n",
    "        # Placeholder: For now assume a text file containing genes x cells\n",
    "        print(f\"this is  {data_path}\")\n",
    "        adata = sc.read_text(data_path)\n",
    "        \n",
    "        return adata\n",
    "\n",
    "    def preprocess_data(self, adata):\n",
    "        \"\"\"\n",
    "        Preprocess the data:\n",
    "        - Transpose if necessary\n",
    "        - PCA\n",
    "        - Filter genes/cells\n",
    "        - Compute QC metrics, etc.\n",
    "        \"\"\"\n",
    "        adata = adata.T\n",
    "        adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "        adata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\",\"RPL\"))\n",
    "        adata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n",
    "\n",
    "        sc.pp.calculate_qc_metrics(\n",
    "            adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20], log1p=True\n",
    "        )\n",
    "\n",
    "        # Basic filtering\n",
    "        sc.pp.filter_cells(adata, min_genes=3)\n",
    "\n",
    "        # PCA and neighbors\n",
    "        sc.pp.pca(adata)\n",
    "        sc.pp.neighbors(adata)\n",
    "\n",
    "        # Normalize and log transform\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "\n",
    "        # Find HVGs\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "        return adata\n",
    "\n",
    "    def build_adjacency(self, adata, k):\n",
    "        \"\"\"\n",
    "        Build an adjacency (KNN) graph for cells.\n",
    "        \"\"\"\n",
    "        distances = adata.obsp[\"distances\"]\n",
    "        distances_csr = distances.tocsr()\n",
    "        N = adata.shape[0]\n",
    "        adj_list = []\n",
    "        for i in range(N):\n",
    "            row_indices = distances_csr[i].indices\n",
    "            row_data = distances_csr[i].data\n",
    "            # Pick top k neighbors\n",
    "            k_neighbors = row_indices[row_data.argsort()[:k]]\n",
    "            adj_list.append(k_neighbors)       \n",
    "        A = th.zeros(size=(N, N))\n",
    "        adj_list = np.array(adj_list)\n",
    "        for i in range(adj_list.shape[0]):\n",
    "            for j in range(k):\n",
    "                A[i, adj_list[i][j]] = 1\n",
    "\n",
    "        edge_list = th.zeros(size=(2, N*k))\n",
    "        for i in range(N):\n",
    "            for j in range(k):\n",
    "                edge_list[0, i*k+j] = i\n",
    "                edge_list[1, i*k+j] = adj_list[i][j]        \n",
    "        return A, adj_list,edge_list\n",
    "\n",
    "    def write_hvg_list(self, adata, output_file):\n",
    "        hvgs = adata.var_names[adata.var[\"highly_variable\"]]\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\",\".join(hvgs))\n",
    "        return hvgs\n",
    "\n",
    "    def query_string_db(self, hvgs, output_file):\n",
    "        \"\"\"\n",
    "        Query the STRING database for interactions among the given list of HVGs.\n",
    "        This is pseudo-code. Implement it based on the STRING API.\n",
    "        \"\"\"\n",
    "        # For now, assume string_interactions.tsv is already available.\n",
    "        # In practice, you'd implement an API call here.\n",
    "        pass\n",
    "\n",
    "    def build_gene_graph_and_embed(self, string_file, emb_dim):\n",
    "        \"\"\"\n",
    "        Build the gene graph from string_interactions.tsv and run node2vec.\n",
    "        \"\"\"\n",
    "        edge_list = pd.DataFrame(csv.reader(open(string_file), delimiter=\"\\t\"))\n",
    "        edge_list.columns = edge_list.iloc[0]\n",
    "        edge_list = edge_list.drop(0)\n",
    "\n",
    "        gene_graph = nx.Graph()\n",
    "        gene_graph.add_edges_from(edge_list[[\"node1_string_id\",\"node2_string_id\"]].values)\n",
    "\n",
    "        node2vec_model = Node2Vec(gene_graph, dimensions=emb_dim, walk_length=30, num_walks=200, workers=4)\n",
    "        model = node2vec_model.fit(window=10, min_count=1, batch_words=4)\n",
    "        node_embeddings = model.wv.vectors\n",
    "        return node_embeddings\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Run the entire pipeline:\n",
    "        - Load dataset\n",
    "        - Preprocess\n",
    "        - Build adjacency\n",
    "        - Write HVG list\n",
    "        - Optionally query STRING\n",
    "        - Build gene embeddings\n",
    "        Return adata, adjacency matrix A, and node_embeddings.\n",
    "        \"\"\"\n",
    "        # 1. Load dataset\n",
    "        adata = self.load_dataset(self.dataset, self.data_path)\n",
    "\n",
    "        # 2. Preprocess\n",
    "        adata = self.preprocess_data(adata)\n",
    "\n",
    "        # 3. Build adjacency\n",
    "        A, adj_list,edge_list = self.build_adjacency(adata, self.k)\n",
    "\n",
    "        # 4. Write HVG list\n",
    "        hvgs = self.write_hvg_list(adata, self.output_hvg)\n",
    "\n",
    "        # 5. Optionally query STRING DB\n",
    "        if self.string_query:\n",
    "            self.query_string_db(hvgs, self.string_file)\n",
    "\n",
    "        # 6. Build gene graph and node embeddings\n",
    "        node_embeddings = self.build_gene_graph_and_embed(self.string_file, self.gene_embedding_dim)\n",
    "        return adata, A, node_embeddings,edge_list\n",
    "\n",
    "\n",
    "processor = DataProcessor(\n",
    "    dataset=\"Biase\",\n",
    "    data_path=\"./tests/datastuff/GSE57249_fpkm.txt\",\n",
    "    k=5,\n",
    "    gene_embedding_dim=64,\n",
    "    string_file=\"./tests/datastuff/string_interactions.tsv\",\n",
    "    string_query=False,  # Set to True if you implement the query function\n",
    "    output_hvg=\"./tests/datastuff/highly_variable_genes.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepEmbeddedClustering(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, n_clusters,gene_numwalkers, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Deep Embedded Clustering Model\n",
    "        \n",
    "        Args:\n",
    "        - input_dim (int): Dimension of input features\n",
    "        - hidden_dims (list): List of hidden layer dimensions\n",
    "        - n_clusters (int): Number of clusters\n",
    "        - alpha (float): Hyperparameter for soft assignment\n",
    "        \"\"\"\n",
    "        super(DeepEmbeddedClustering, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(GATConv(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(nn.Linear(hidden_dim, prev_dim))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers = decoder_layers[::-1]\n",
    "        self.encoder = encoder_layers\n",
    "        self.decoder = decoder_layers\n",
    "\n",
    "        self.genedecoder = nn.Linear(input_dim, gene_numwalkers)\n",
    "        \n",
    "\n",
    "        # Clustering layer\n",
    "        self.clustering_layer = nn.Linear(hidden_dims[-1], n_clusters, bias=False)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.alpha = alpha\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def encode(self, x, graphedge_list):\n",
    "        \"\"\"Forward pass through the encoder\"\"\"\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, graphedge_list)\n",
    "        return x\n",
    "    def decode(self, x, gene_embeddings):\n",
    "        \"\"\"Forward pass through the decoder\n",
    "        gene embedding is of size (n_genes, hidden_dim) , while x is of size (n_cells, hidden_dim), so we append it.\n",
    "        And the final x size is (n_cells+n_genes, hidden_dim)\"\"\"\n",
    "        n_cells = x.size(0)\n",
    "        x = th.cat((x,gene_embeddings),0)\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "        ''' Our output will be of size (n_cells+n_genes, input_dim), so we need to split it into two parts, one for cells and one for genes'''\n",
    "        x_cells = x[:n_cells]\n",
    "        x_genes = x[n_cells:]\n",
    "        x_genes = self.genedecoder(x_genes)\n",
    "        return x_cells, x_genes\n",
    "     \n",
    "    def forward(self, x,graph_edge_list):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        z = self.encode(x,graph_edge_list)\n",
    "        q = self._soft_assignment(z)\n",
    "        return z, q\n",
    "    def _soft_assignment(self, z):\n",
    "        \"\"\"Compute soft assignment probabilities\"\"\"\n",
    "        # Compute similarity between embedded points and cluster centers\n",
    "        weights = self.clustering_layer.weight\n",
    "        q = 1.0 / ((1.0 + th.sum((z.unsqueeze(1) - weights)**2, dim=2) / self.alpha) + 1e-8) ** ((self.alpha + 1.0) / 2.0)\n",
    "        q = q / th.sum(q, dim=1, keepdim=True)\n",
    "        return q\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        \"\"\"\n",
    "        Compute target distribution (sharpened version of q)\n",
    "        \n",
    "        Args:\n",
    "        - q (th.Tensor): Soft assignment probabilities\n",
    "        \n",
    "        Returns:\n",
    "        - p (th.Tensor): Sharpened target distribution\n",
    "        \"\"\"\n",
    "        p = q**2 / th.sum(q, dim=0)\n",
    "        p = p / th.sum(p, dim=1, keepdim=True)\n",
    "        return p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_autoencoder(model, data,gene_embeddings,edge_list, epochs=50, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Pretrain autoencoder for initial feature extraction\n",
    "    \n",
    "    Args:\n",
    "    - model (DeepEmbeddedClustering): DEC model\n",
    "    - data (th.Tensor): Input data\n",
    "    - epochs (int): Number of pretraining epochs\n",
    "    - lr (float): Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - Pretrained model\n",
    "    \"\"\"\n",
    "    cosine_loss = nn.CosineSimilarity()\n",
    "    mae_loss = nn.L1Loss()\n",
    "    total_loss = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(data,edge_list)\n",
    "        x_cells,x_genes = model.decode(z,gene_embeddings)\n",
    "        loss = -1*th.sum(cosine_loss(data,x_cells)) + mae_loss(x_genes,gene_embeddings)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Pretraining Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dec_loss(p, q):\n",
    "    \"\"\"\n",
    "    Compute DEC loss (KL divergence between p and q)\n",
    "    \n",
    "    Args:\n",
    "    - p (th.Tensor): Target distribution\n",
    "    - q (th.Tensor): Soft assignment probabilities\n",
    "    \n",
    "    Returns:\n",
    "    - loss (th.Tensor): KL divergence loss\n",
    "    \"\"\"\n",
    "    return th.mean(th.sum(p * th.log(p / q), dim=1))\n",
    "\n",
    "def train_dec(model,edge_list, data, epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train Deep Embedded Clustering model\n",
    "    \n",
    "    Args:\n",
    "    - model (DeepEmbeddedClustering): DEC model\n",
    "    - data (th.Tensor): Input data\n",
    "    - epochs (int): Number of training epochs\n",
    "    - lr (float): Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model and cluster assignments\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize cluster centers using KMeans\n",
    "    with th.no_grad():\n",
    "        z = model.encode(data,edge_list)\n",
    "        kmeans = KMeans(n_clusters=model.n_clusters, n_init=20)\n",
    "        cluster_labels = kmeans.fit_predict(z.numpy())\n",
    "        \n",
    "        # Set initial cluster centers\n",
    "        model.clustering_layer.weight.copy_(\n",
    "            th.from_numpy(kmeans.cluster_centers_).float()\n",
    "        )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        z, q = model(data,edge_list)\n",
    "        \n",
    "        # Compute target distribution\n",
    "        p = model.target_distribution(q)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = dec_loss(p, q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'DEC Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Get final cluster assignments\n",
    "    _, q = model(data,edge_list)\n",
    "    cluster_assignments = th.argmax(q, dim=1)\n",
    "    \n",
    "    return model, cluster_assignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is  ./datasets/Biase/GSE57249_fpkm.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\scanpy\\preprocessing\\_highly_variable_genes.py:305: RuntimeWarning: invalid value encountered in log\n",
      "  dispersion = np.log(dispersion)\n",
      "Computing transition probabilities: 100%|██████████| 647/647 [00:00<00:00, 5343.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simulated data\n",
    "processor = DataProcessor(\n",
    "    dataset=\"Biase\",\n",
    "    data_path=\"./datasets/Biase/GSE57249_fpkm.txt\",\n",
    "    k=5,\n",
    "    gene_embedding_dim=64,\n",
    "    string_file=\"./datasets/Biase/string_interactions.tsv\",\n",
    "    string_query=False,  # Set to True if you implement the query function\n",
    "    output_hvg=\"./datasets/Biase/highly_variable_genes.txt\"\n",
    ")\n",
    "adata, A, node_embeddings,edge_list = processor.get_data()\n",
    "# so our input is the sc matrix of only the hvg genes\n",
    "hvg = adata.var['highly_variable']\n",
    "adata_hvg = adata[:, hvg].X\n",
    "data = th.tensor(adata_hvg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data.shape[1]\n",
    "hidden_dims = [256, 128, 64]\n",
    "n_clusters = 3\n",
    "gene_numwalkers = node_embeddings.shape[1]\n",
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Epoch [0/500], Loss: 0.8628\n",
      "Pretraining Epoch [10/500], Loss: 0.6943\n",
      "Pretraining Epoch [20/500], Loss: 0.6389\n",
      "Pretraining Epoch [30/500], Loss: 0.6056\n",
      "Pretraining Epoch [40/500], Loss: 0.5822\n",
      "Pretraining Epoch [50/500], Loss: 0.5645\n",
      "Pretraining Epoch [60/500], Loss: 0.5508\n",
      "Pretraining Epoch [70/500], Loss: 0.5408\n",
      "Pretraining Epoch [80/500], Loss: 0.5347\n",
      "Pretraining Epoch [90/500], Loss: 0.5286\n",
      "Pretraining Epoch [100/500], Loss: 0.5284\n",
      "Pretraining Epoch [110/500], Loss: 0.5266\n",
      "Pretraining Epoch [120/500], Loss: 0.5250\n",
      "Pretraining Epoch [130/500], Loss: 0.5298\n",
      "Pretraining Epoch [140/500], Loss: 0.5330\n",
      "Pretraining Epoch [150/500], Loss: 0.5311\n",
      "Pretraining Epoch [160/500], Loss: 0.5305\n",
      "Pretraining Epoch [170/500], Loss: 0.5276\n",
      "Pretraining Epoch [180/500], Loss: 0.5283\n",
      "Pretraining Epoch [190/500], Loss: 0.5277\n",
      "Pretraining Epoch [200/500], Loss: 0.5266\n",
      "Pretraining Epoch [210/500], Loss: 0.5237\n",
      "Pretraining Epoch [220/500], Loss: 0.5253\n",
      "Pretraining Epoch [230/500], Loss: 0.5257\n",
      "Pretraining Epoch [240/500], Loss: 0.5239\n",
      "Pretraining Epoch [250/500], Loss: 0.5232\n",
      "Pretraining Epoch [260/500], Loss: 0.5235\n",
      "Pretraining Epoch [270/500], Loss: 0.5228\n",
      "Pretraining Epoch [280/500], Loss: 0.5225\n",
      "Pretraining Epoch [290/500], Loss: 0.5252\n",
      "Pretraining Epoch [300/500], Loss: 0.5223\n",
      "Pretraining Epoch [310/500], Loss: 0.5235\n",
      "Pretraining Epoch [320/500], Loss: 0.5230\n",
      "Pretraining Epoch [330/500], Loss: 0.5219\n",
      "Pretraining Epoch [340/500], Loss: 0.5228\n",
      "Pretraining Epoch [350/500], Loss: 0.5230\n",
      "Pretraining Epoch [360/500], Loss: 0.5203\n",
      "Pretraining Epoch [370/500], Loss: 0.5237\n",
      "Pretraining Epoch [380/500], Loss: 0.5224\n",
      "Pretraining Epoch [390/500], Loss: 0.5215\n",
      "Pretraining Epoch [400/500], Loss: 0.5230\n",
      "Pretraining Epoch [410/500], Loss: 0.5238\n",
      "Pretraining Epoch [420/500], Loss: 0.5219\n",
      "Pretraining Epoch [430/500], Loss: 0.5249\n",
      "Pretraining Epoch [440/500], Loss: 0.5244\n",
      "Pretraining Epoch [450/500], Loss: 0.5219\n",
      "Pretraining Epoch [460/500], Loss: 0.5223\n",
      "Pretraining Epoch [470/500], Loss: 0.5243\n",
      "Pretraining Epoch [480/500], Loss: 0.5222\n",
      "Pretraining Epoch [490/500], Loss: 0.5235\n",
      "DEC Epoch [0/2000], Loss: 0.0741\n",
      "DEC Epoch [10/2000], Loss: 0.0739\n",
      "DEC Epoch [20/2000], Loss: 0.0736\n",
      "DEC Epoch [30/2000], Loss: 0.0732\n",
      "DEC Epoch [40/2000], Loss: 0.0729\n",
      "DEC Epoch [50/2000], Loss: 0.0725\n",
      "DEC Epoch [60/2000], Loss: 0.0720\n",
      "DEC Epoch [70/2000], Loss: 0.0716\n",
      "DEC Epoch [80/2000], Loss: 0.0711\n",
      "DEC Epoch [90/2000], Loss: 0.0706\n",
      "DEC Epoch [100/2000], Loss: 0.0701\n",
      "DEC Epoch [110/2000], Loss: 0.0696\n",
      "DEC Epoch [120/2000], Loss: 0.0690\n",
      "DEC Epoch [130/2000], Loss: 0.0685\n",
      "DEC Epoch [140/2000], Loss: 0.0680\n",
      "DEC Epoch [150/2000], Loss: 0.0674\n",
      "DEC Epoch [160/2000], Loss: 0.0668\n",
      "DEC Epoch [170/2000], Loss: 0.0663\n",
      "DEC Epoch [180/2000], Loss: 0.0657\n",
      "DEC Epoch [190/2000], Loss: 0.0652\n",
      "DEC Epoch [200/2000], Loss: 0.0646\n",
      "DEC Epoch [210/2000], Loss: 0.0641\n",
      "DEC Epoch [220/2000], Loss: 0.0635\n",
      "DEC Epoch [230/2000], Loss: 0.0630\n",
      "DEC Epoch [240/2000], Loss: 0.0624\n",
      "DEC Epoch [250/2000], Loss: 0.0619\n",
      "DEC Epoch [260/2000], Loss: 0.0614\n",
      "DEC Epoch [270/2000], Loss: 0.0609\n",
      "DEC Epoch [280/2000], Loss: 0.0604\n",
      "DEC Epoch [290/2000], Loss: 0.0599\n",
      "DEC Epoch [300/2000], Loss: 0.0594\n",
      "DEC Epoch [310/2000], Loss: 0.0589\n",
      "DEC Epoch [320/2000], Loss: 0.0585\n",
      "DEC Epoch [330/2000], Loss: 0.0580\n",
      "DEC Epoch [340/2000], Loss: 0.0576\n",
      "DEC Epoch [350/2000], Loss: 0.0572\n",
      "DEC Epoch [360/2000], Loss: 0.0568\n",
      "DEC Epoch [370/2000], Loss: 0.0564\n",
      "DEC Epoch [380/2000], Loss: 0.0560\n",
      "DEC Epoch [390/2000], Loss: 0.0557\n",
      "DEC Epoch [400/2000], Loss: 0.0553\n",
      "DEC Epoch [410/2000], Loss: 0.0550\n",
      "DEC Epoch [420/2000], Loss: 0.0546\n",
      "DEC Epoch [430/2000], Loss: 0.0543\n",
      "DEC Epoch [440/2000], Loss: 0.0540\n",
      "DEC Epoch [450/2000], Loss: 0.0537\n",
      "DEC Epoch [460/2000], Loss: 0.0534\n",
      "DEC Epoch [470/2000], Loss: 0.0532\n",
      "DEC Epoch [480/2000], Loss: 0.0529\n",
      "DEC Epoch [490/2000], Loss: 0.0527\n",
      "DEC Epoch [500/2000], Loss: 0.0525\n",
      "DEC Epoch [510/2000], Loss: 0.0522\n",
      "DEC Epoch [520/2000], Loss: 0.0520\n",
      "DEC Epoch [530/2000], Loss: 0.0518\n",
      "DEC Epoch [540/2000], Loss: 0.0516\n",
      "DEC Epoch [550/2000], Loss: 0.0514\n",
      "DEC Epoch [560/2000], Loss: 0.0513\n",
      "DEC Epoch [570/2000], Loss: 0.0511\n",
      "DEC Epoch [580/2000], Loss: 0.0510\n",
      "DEC Epoch [590/2000], Loss: 0.0508\n",
      "DEC Epoch [600/2000], Loss: 0.0507\n",
      "DEC Epoch [610/2000], Loss: 0.0505\n",
      "DEC Epoch [620/2000], Loss: 0.0504\n",
      "DEC Epoch [630/2000], Loss: 0.0503\n",
      "DEC Epoch [640/2000], Loss: 0.0502\n",
      "DEC Epoch [650/2000], Loss: 0.0501\n",
      "DEC Epoch [660/2000], Loss: 0.0500\n",
      "DEC Epoch [670/2000], Loss: 0.0499\n",
      "DEC Epoch [680/2000], Loss: 0.0498\n",
      "DEC Epoch [690/2000], Loss: 0.0498\n",
      "DEC Epoch [700/2000], Loss: 0.0497\n",
      "DEC Epoch [710/2000], Loss: 0.0496\n",
      "DEC Epoch [720/2000], Loss: 0.0495\n",
      "DEC Epoch [730/2000], Loss: 0.0495\n",
      "DEC Epoch [740/2000], Loss: 0.0494\n",
      "DEC Epoch [750/2000], Loss: 0.0494\n",
      "DEC Epoch [760/2000], Loss: 0.0493\n",
      "DEC Epoch [770/2000], Loss: 0.0493\n",
      "DEC Epoch [780/2000], Loss: 0.0492\n",
      "DEC Epoch [790/2000], Loss: 0.0492\n",
      "DEC Epoch [800/2000], Loss: 0.0492\n",
      "DEC Epoch [810/2000], Loss: 0.0491\n",
      "DEC Epoch [820/2000], Loss: 0.0491\n",
      "DEC Epoch [830/2000], Loss: 0.0491\n",
      "DEC Epoch [840/2000], Loss: 0.0491\n",
      "DEC Epoch [850/2000], Loss: 0.0490\n",
      "DEC Epoch [860/2000], Loss: 0.0490\n",
      "DEC Epoch [870/2000], Loss: 0.0490\n",
      "DEC Epoch [880/2000], Loss: 0.0490\n",
      "DEC Epoch [890/2000], Loss: 0.0490\n",
      "DEC Epoch [900/2000], Loss: 0.0490\n",
      "DEC Epoch [910/2000], Loss: 0.0489\n",
      "DEC Epoch [920/2000], Loss: 0.0489\n",
      "DEC Epoch [930/2000], Loss: 0.0489\n",
      "DEC Epoch [940/2000], Loss: 0.0489\n",
      "DEC Epoch [950/2000], Loss: 0.0489\n",
      "DEC Epoch [960/2000], Loss: 0.0489\n",
      "DEC Epoch [970/2000], Loss: 0.0489\n",
      "DEC Epoch [980/2000], Loss: 0.0489\n",
      "DEC Epoch [990/2000], Loss: 0.0489\n",
      "DEC Epoch [1000/2000], Loss: 0.0489\n",
      "DEC Epoch [1010/2000], Loss: 0.0489\n",
      "DEC Epoch [1020/2000], Loss: 0.0489\n",
      "DEC Epoch [1030/2000], Loss: 0.0489\n",
      "DEC Epoch [1040/2000], Loss: 0.0489\n",
      "DEC Epoch [1050/2000], Loss: 0.0489\n",
      "DEC Epoch [1060/2000], Loss: 0.0489\n",
      "DEC Epoch [1070/2000], Loss: 0.0489\n",
      "DEC Epoch [1080/2000], Loss: 0.0490\n",
      "DEC Epoch [1090/2000], Loss: 0.0490\n",
      "DEC Epoch [1100/2000], Loss: 0.0490\n",
      "DEC Epoch [1110/2000], Loss: 0.0490\n",
      "DEC Epoch [1120/2000], Loss: 0.0490\n",
      "DEC Epoch [1130/2000], Loss: 0.0490\n",
      "DEC Epoch [1140/2000], Loss: 0.0490\n",
      "DEC Epoch [1150/2000], Loss: 0.0490\n",
      "DEC Epoch [1160/2000], Loss: 0.0490\n",
      "DEC Epoch [1170/2000], Loss: 0.0490\n",
      "DEC Epoch [1180/2000], Loss: 0.0490\n",
      "DEC Epoch [1190/2000], Loss: 0.0491\n",
      "DEC Epoch [1200/2000], Loss: 0.0491\n",
      "DEC Epoch [1210/2000], Loss: 0.0491\n",
      "DEC Epoch [1220/2000], Loss: 0.0491\n",
      "DEC Epoch [1230/2000], Loss: 0.0491\n",
      "DEC Epoch [1240/2000], Loss: 0.0491\n",
      "DEC Epoch [1250/2000], Loss: 0.0491\n",
      "DEC Epoch [1260/2000], Loss: 0.0491\n",
      "DEC Epoch [1270/2000], Loss: 0.0492\n",
      "DEC Epoch [1280/2000], Loss: 0.0492\n",
      "DEC Epoch [1290/2000], Loss: 0.0492\n",
      "DEC Epoch [1300/2000], Loss: 0.0492\n",
      "DEC Epoch [1310/2000], Loss: 0.0492\n",
      "DEC Epoch [1320/2000], Loss: 0.0492\n",
      "DEC Epoch [1330/2000], Loss: 0.0492\n",
      "DEC Epoch [1340/2000], Loss: 0.0492\n",
      "DEC Epoch [1350/2000], Loss: 0.0493\n",
      "DEC Epoch [1360/2000], Loss: 0.0493\n",
      "DEC Epoch [1370/2000], Loss: 0.0493\n",
      "DEC Epoch [1380/2000], Loss: 0.0493\n",
      "DEC Epoch [1390/2000], Loss: 0.0493\n",
      "DEC Epoch [1400/2000], Loss: 0.0493\n",
      "DEC Epoch [1410/2000], Loss: 0.0493\n",
      "DEC Epoch [1420/2000], Loss: 0.0494\n",
      "DEC Epoch [1430/2000], Loss: 0.0494\n",
      "DEC Epoch [1440/2000], Loss: 0.0494\n",
      "DEC Epoch [1450/2000], Loss: 0.0494\n",
      "DEC Epoch [1460/2000], Loss: 0.0494\n",
      "DEC Epoch [1470/2000], Loss: 0.0494\n",
      "DEC Epoch [1480/2000], Loss: 0.0494\n",
      "DEC Epoch [1490/2000], Loss: 0.0494\n",
      "DEC Epoch [1500/2000], Loss: 0.0495\n",
      "DEC Epoch [1510/2000], Loss: 0.0495\n",
      "DEC Epoch [1520/2000], Loss: 0.0495\n",
      "DEC Epoch [1530/2000], Loss: 0.0495\n",
      "DEC Epoch [1540/2000], Loss: 0.0495\n",
      "DEC Epoch [1550/2000], Loss: 0.0495\n",
      "DEC Epoch [1560/2000], Loss: 0.0495\n",
      "DEC Epoch [1570/2000], Loss: 0.0495\n",
      "DEC Epoch [1580/2000], Loss: 0.0495\n",
      "DEC Epoch [1590/2000], Loss: 0.0496\n",
      "DEC Epoch [1600/2000], Loss: 0.0496\n",
      "DEC Epoch [1610/2000], Loss: 0.0496\n",
      "DEC Epoch [1620/2000], Loss: 0.0496\n",
      "DEC Epoch [1630/2000], Loss: 0.0496\n",
      "DEC Epoch [1640/2000], Loss: 0.0496\n",
      "DEC Epoch [1650/2000], Loss: 0.0496\n",
      "DEC Epoch [1660/2000], Loss: 0.0496\n",
      "DEC Epoch [1670/2000], Loss: 0.0497\n",
      "DEC Epoch [1680/2000], Loss: 0.0497\n",
      "DEC Epoch [1690/2000], Loss: 0.0497\n",
      "DEC Epoch [1700/2000], Loss: 0.0497\n",
      "DEC Epoch [1710/2000], Loss: 0.0497\n",
      "DEC Epoch [1720/2000], Loss: 0.0497\n",
      "DEC Epoch [1730/2000], Loss: 0.0497\n",
      "DEC Epoch [1740/2000], Loss: 0.0497\n",
      "DEC Epoch [1750/2000], Loss: 0.0497\n",
      "DEC Epoch [1760/2000], Loss: 0.0497\n",
      "DEC Epoch [1770/2000], Loss: 0.0498\n",
      "DEC Epoch [1780/2000], Loss: 0.0498\n",
      "DEC Epoch [1790/2000], Loss: 0.0498\n",
      "DEC Epoch [1800/2000], Loss: 0.0498\n",
      "DEC Epoch [1810/2000], Loss: 0.0498\n",
      "DEC Epoch [1820/2000], Loss: 0.0498\n",
      "DEC Epoch [1830/2000], Loss: 0.0498\n",
      "DEC Epoch [1840/2000], Loss: 0.0498\n",
      "DEC Epoch [1850/2000], Loss: 0.0498\n",
      "DEC Epoch [1860/2000], Loss: 0.0498\n",
      "DEC Epoch [1870/2000], Loss: 0.0498\n",
      "DEC Epoch [1880/2000], Loss: 0.0499\n",
      "DEC Epoch [1890/2000], Loss: 0.0499\n",
      "DEC Epoch [1900/2000], Loss: 0.0499\n",
      "DEC Epoch [1910/2000], Loss: 0.0499\n",
      "DEC Epoch [1920/2000], Loss: 0.0499\n",
      "DEC Epoch [1930/2000], Loss: 0.0499\n",
      "DEC Epoch [1940/2000], Loss: 0.0499\n",
      "DEC Epoch [1950/2000], Loss: 0.0499\n",
      "DEC Epoch [1960/2000], Loss: 0.0499\n",
      "DEC Epoch [1970/2000], Loss: 0.0499\n",
      "DEC Epoch [1980/2000], Loss: 0.0499\n",
      "DEC Epoch [1990/2000], Loss: 0.0499\n",
      "Clustering complete. Cluster labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
      "        2, 2, 2, 0, 2, 0, 0, 2])\n",
      "Cluster labels from DEC model: [1 1 1 1 1 1 1 1 1 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 0 0 2]\n",
      "Cluster labels from sklearn KMeans: [2 2 2 2 2 2 0 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model = DeepEmbeddedClustering(input_dim, hidden_dims, n_clusters,gene_numwalkers)\n",
    "edge_list = edge_list.int()\n",
    "pretrain_autoencoder(model,data,th.tensor(node_embeddings),edge_list,epochs=500,lr=2e-3)\n",
    "\n",
    "# Train model\n",
    "trained_model, cluster_labels = train_dec(model,edge_list ,data,epochs=2000,lr=1e-3)\n",
    "\n",
    "print(\"Clustering complete. Cluster labels:\", cluster_labels)\n",
    "\n",
    "# Compare with sklearn clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.numpy())\n",
    "\n",
    "# Fit KMeans from sklearn\n",
    "kmeans_sklearn = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "cluster_labels_sklearn = kmeans_sklearn.fit_predict(data_scaled)\n",
    "\n",
    "# Print comparison\n",
    "print(\"Cluster labels from DEC model:\", cluster_labels.numpy())\n",
    "print(\"Cluster labels from sklearn KMeans:\", cluster_labels_sklearn)\n",
    "\n",
    "# Compute ARI and NMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes = pd.read_csv(\"./datasets/Biase/subtype.ann\",delimiter=\"\\t\",header=None)\n",
    "celltypes.columns = ['cell','type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.3012\n",
      "Normalized Mutual Information (NMI): 0.4388\n",
      "Sklearn ARI and NMI:\n",
      "Adjusted Rand Index (ARI): 0.3746\n",
      "Normalized Mutual Information (NMI): 0.6023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ari = adjusted_rand_score(cluster_labels[:49],celltypes.values[:,1][1:])\n",
    "nmi = normalized_mutual_info_score(cluster_labels[:49],celltypes.values[:,1][1:])\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "\n",
    "print(\"Sklearn ARI and NMI:\")\n",
    "ari = adjusted_rand_score(cluster_labels_sklearn[:49],celltypes.values[:,1][1:])\n",
    "nmi = normalized_mutual_info_score(cluster_labels_sklearn[:49],celltypes.values[:,1][1:])\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = th.randn(100, 128)\n",
    "input2 = th.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.0687e-02, -3.8181e-02,  1.0573e-01,  6.7259e-02,  1.7614e-02,\n",
       "        -1.7498e-01, -5.8014e-02,  3.4063e-02,  1.7652e-02, -6.9140e-02,\n",
       "        -2.9786e-02, -1.0883e-01, -8.3510e-02,  1.9375e-01, -3.4685e-02,\n",
       "         7.4517e-02,  6.4099e-02,  7.9769e-02, -3.9772e-02, -9.3325e-02,\n",
       "        -5.3324e-02,  1.0575e-01, -5.2770e-02, -2.6001e-03,  1.2925e-01,\n",
       "         4.9770e-02,  1.0250e-02, -1.3765e-01, -2.3494e-01,  1.2739e-02,\n",
       "        -1.1573e-01, -8.1230e-02, -1.0116e-01,  1.3597e-01, -1.1521e-01,\n",
       "        -6.0887e-02, -1.6510e-02,  1.5036e-01,  1.1067e-02,  5.6749e-02,\n",
       "        -1.4481e-01, -1.5417e-02, -2.6879e-01, -2.0842e-02,  7.9560e-02,\n",
       "        -6.4373e-03,  1.1987e-01, -1.1367e-01, -6.6169e-02, -4.3092e-02,\n",
       "         2.6214e-01,  4.9591e-02, -1.0859e-01, -2.7429e-02, -1.9571e-02,\n",
       "         1.1765e-01,  5.8556e-02, -1.1562e-01,  4.0295e-02,  2.2053e-01,\n",
       "         4.1145e-02, -1.3348e-02,  1.3305e-02, -3.6769e-03,  5.0300e-02,\n",
       "        -5.6241e-02,  2.3380e-02,  4.1098e-02, -7.4883e-04,  8.2624e-02,\n",
       "         4.8768e-03, -7.9367e-03,  1.2334e-01,  7.0835e-02,  8.1938e-02,\n",
       "        -4.9358e-02, -6.6963e-02, -1.6121e-01,  4.6227e-02, -1.4750e-02,\n",
       "        -1.0139e-01, -7.8377e-02, -1.3258e-01, -3.0089e-02,  7.6311e-02,\n",
       "         3.7953e-02,  6.7579e-02, -6.8158e-02,  1.4273e-01,  8.4220e-02,\n",
       "        -3.2778e-02,  9.7557e-02, -8.3739e-02,  2.1598e-01,  8.7961e-02,\n",
       "         5.9706e-02, -3.7865e-02, -9.9955e-02, -2.3957e-02, -8.9020e-02,\n",
       "         9.5837e-02,  1.4074e-01,  5.1027e-02,  6.1799e-02,  1.0528e-01,\n",
       "         4.6320e-02,  7.3747e-02, -3.1465e-02,  1.3457e-01,  2.1209e-01,\n",
       "        -5.8328e-02,  3.1899e-02, -1.1168e-01, -1.0669e-01,  6.8869e-02,\n",
       "        -1.5783e-01, -2.1631e-02, -3.8142e-02,  2.8665e-02,  1.7429e-01,\n",
       "        -9.2896e-02, -1.0590e-01,  4.3213e-02,  1.9107e-01,  9.8033e-02,\n",
       "         6.9714e-03,  1.5027e-04,  4.0953e-02])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ari = adjusted_rand_score(cluster_labels.numpy())\n",
    "nmi = normalized_mutual_info_score(cluster_labels.numpy)\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
