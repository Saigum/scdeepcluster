{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader,NodeLoader\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch as th\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphattention_layer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,adjM):\n",
    "        '''WkH_{i-1} is of dimension : CurrentNodeShape x N'''  \n",
    "        self.inpshape = input_size\n",
    "        self.opshape = output_size\n",
    "        self.A = adjM\n",
    "        super(graphattention_layer,self).__init__()\n",
    "        self.vks = nn.Linear(in_features=output_size,out_features=  1)\n",
    "        self.vkr = nn.Linear(in_features=output_size,out_features= 1)\n",
    "        self.W =  nn.Linear(in_features=input_size,out_features=output_size) \n",
    "    def forward(self, H_k,A):\n",
    "        '''H_k represents the previous layer's graph representation'''\n",
    "        '''So i have to account for subgraph forward passes,?'''\n",
    "        if(A is None):\n",
    "            M_s = self.A * self.vks(F.relu(self.W(H_k))).T\n",
    "            M_r = (self.A * self.vkr(F.relu(self.W(H_k))).T).T\n",
    "            Attention = F.softmax(F.sigmoid(M_s+M_r))\n",
    "            H_new = Attention@F.relu(self.W(H_k))\n",
    "            return H_new\n",
    "        else:\n",
    "            ''' To allow you to pass through subgraphs, in my attempt for batching'''\n",
    "            M_s = A * self.vks(F.relu(self.W(H_k))).T\n",
    "            M_r = (A * self.vkr(F.relu(self.W(H_k))).T).T\n",
    "            Attention = F.softmax(F.sigmoid(M_s+M_r))\n",
    "            H_new = Attention@F.relu(self.W(H_k))\n",
    "            return H_new  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,adjM,input_embeddings):\n",
    "        super(encoder,self).__init__( )\n",
    "        ''' \n",
    "        remember that in pytorch, your input_size is the last dimension of your input\n",
    "        So when my input is F*N, input_size = F\n",
    "        also a row in my matrix corresponds to a cell's representation\n",
    "        '''\n",
    "        ''' For scDEGA, the encoder block remains the same, but the decoder block changes'''\n",
    "        ''' Input embeddings is the embedding vector size for one cell.'''\n",
    "        self.layer1 = graphattention_layer(input_size=input_embeddings\n",
    "                                           ,output_size=512\n",
    "                                           ,adjM=adjM)\n",
    "        self.layer2 = graphattention_layer(input_size=512\n",
    "                                           ,output_size=256\n",
    "                                           ,adjM=adjM)\n",
    "        self.layer3 = graphattention_layer(input_size=256\n",
    "                                           ,output_size=64\n",
    "                                           ,adjM=adjM)\n",
    "    def forward(self, X,A):\n",
    "        '''\n",
    "        X here is the node embeddings, its of shape (N*embedding_size)\n",
    "        I'm gonna tranpose it once in the start, and then at the end.\n",
    "        H3 is of size N*64\n",
    "        '''\n",
    "        H1 = self.layer1(X,A)\n",
    "        H2 = self.layer2(H1,A)\n",
    "        H3 = self.layer3(H2,A)\n",
    "        return H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class gene_decoder(nn.Module):\n",
    "    def __init__(self,adjM,reconstruction_embedding,gene_embeddings):\n",
    "        ''' Reconstruction embedding is the size that your decoded embedding should be.\n",
    "        Simply said, it's the original embedding size for our genes.'''\n",
    "        super(gene_decoder,self).__init__()\n",
    "        self.gene_embeddings = gene_embeddings\n",
    "        gene_embedding_size = gene_embeddings.shape[1] # embedding of a single gene.\n",
    "        ''' Assuming gene_embeddings is 64 * 647,\n",
    "        (647 being the number of genes there) so embedding size is shape[1])'''\n",
    "        self.cell_layer1 = graphattention_layer(input_size=64,\n",
    "                                           output_size=256,adjM=adjM)\n",
    "        self.cell_layer2 = graphattention_layer(input_size=256,\n",
    "                                           output_size=512,adjM=adjM)\n",
    "        self.cell_layer3 = graphattention_layer(input_size=512,\n",
    "                                           output_size=reconstruction_embedding,adjM=adjM)\n",
    "        self.Wcr = nn.Parameter(\n",
    "            th.randn(reconstruction_embedding,reconstruction_embedding+gene_embedding_size))\n",
    "        self.Wgr = nn.Parameter(\n",
    "            th.randn(gene_embedding_size,gene_embedding_size+reconstruction_embedding))\n",
    "    def forward(self, H,A):\n",
    "        '''\n",
    "        H here is the encoder's output\n",
    "        I'm gonna stack the gene embeddings to the H matrix\n",
    "        Encoder should have returned a 64*N matrix\n",
    "        Gene embeddings should be of dimension num_nodes*64 , which was 647 for the first run.\n",
    "        So we're concatenating a 647*64 matrix to a N*64 matrix\n",
    "        '''\n",
    "        # Hpass is a 64*(N+647) matrix\n",
    "        Hpass = th.stack(H,self.gene_embeddings) \n",
    "        # We're passing in the (N+647)*64 matrix \n",
    "        cell_H1 = self.layer1(Hpass,A)\n",
    "        cell_H2 = self.layer2(cell_H1,A)\n",
    "        cell_H3= self.layer3(cell_H2,A)\n",
    "        X_cr = self.Wcr@cell_H3 # cell reconstructed matrix.\n",
    "        X_gr = self.Wgr@Hpass\n",
    "        ''' H3 would be of size N*N'''\n",
    "        return (X_cr,X_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clustering_layer(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ''' This is the clustering optimization layer for my model.\n",
    "        The input will be my cell embeddings, and the output will be the cluster assignments.\n",
    "        Initial cluster assignments will be based on the kmeans algorithm.\n",
    "        Each step, I'm going to update the cluster assignments based on the cell embeddings.\n",
    "        Basically, I'm going to \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        super(clustering_layer,self).__init__()\n",
    "    def forward(self, H):\n",
    "        ''' The size of the Q matrix,\n",
    "        will be N*C, where N is the number of nodes, and C is the number of clusters.'''\n",
    "        \n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scdEGA(nn.Module):\n",
    "    def __init__(self,hidden_size,cellGraph,adjM,GeneGraph):\n",
    "        '''GeneGraph is the gene embeddings from the node2vec model\n",
    "           run on a PPI graph constructed from the gene interactions.\n",
    "           This will be the gene matrix (so gene loss) we wish to reconstruct.\n",
    "                      \n",
    "           Cellmatrix_pca is the PCA reduced cell matrix.\n",
    "           This will be the cell matrix (so cell loss) we wish to reconstruct.\n",
    "\n",
    "           adjM is the adjacency matrix of the cell graph.\n",
    "        '''\n",
    "        super(scdEGA,self).__init__()\n",
    "        self.gc = cellGraph\n",
    "        self.gg = GeneGraph\n",
    "        cell_embeddingsize = cellGraph.shape[1]\n",
    "        gene_embeddingsize = GeneGraph.shape[1]\n",
    "        self.encoder = encoder(adjM,cell_embeddingsize)\n",
    "        self.decoder = gene_decoder(adjM,gene_embeddingsize,GeneGraph)\n",
    "        self.clustering_layer = clustering_layer(hidden_size)\n",
    "    def forward(self, H,A=None):\n",
    "        cell_embeddings = self.encoder(H,A)\n",
    "        reconstructed_cell_matrix,reconstructed_gene_matrix = self.decoder(cell_embeddings,A)\n",
    "        self.reconstruction_cell_loss = F.cosine_similarity(reconstructed_cell_matrix,self.gc)\n",
    "        self.reconstruction_gene_loss = F.mean_absolute_error(reconstructed_gene_matrix,self.gg)\n",
    "        self.selfsupervised_loss = self.reconstruction_cell_loss+self.reconstruction_gene_loss\n",
    "        return reconstructed_cell_matrix,reconstructed_gene_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
