{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader,NodeLoader\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch as th\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphattention_layer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,adjM):\n",
    "        '''WkH_{i-1} is of dimension : CurrentNodeShape x N'''  \n",
    "        self.inpshape = input_size\n",
    "        self.opshape = output_size\n",
    "        self.A = adjM\n",
    "        super(graphattention_layer,self).__init__()\n",
    "        self.vks = nn.Linear(in_features=output_size,out_features=  1)\n",
    "        self.vkr = nn.Linear(in_features=output_size,out_features= 1)\n",
    "        self.W =  nn.Linear(in_features=input_size,out_features=output_size) \n",
    "    def forward(self, H_k,A):\n",
    "        '''H_k represents the previous layer's graph representation'''\n",
    "        '''So i have to account for subgraph forward passes,?'''\n",
    "        if(A is None):\n",
    "            M_s = self.A * self.vks(F.relu(self.W(H_k))).T\n",
    "            M_r = (self.A * self.vkr(F.relu(self.W(H_k))).T).T\n",
    "            Attention = F.softmax(F.sigmoid(M_s+M_r))\n",
    "            H_new = Attention@F.relu(self.W(H_k))\n",
    "            return H_new\n",
    "        else:\n",
    "            ''' To allow you to pass through subgraphs, in my attempt for batching'''\n",
    "            M_s = A * self.vks(F.relu(self.W(H_k))).T\n",
    "            M_r = (A * self.vkr(F.relu(self.W(H_k))).T).T\n",
    "            Attention = F.softmax(F.sigmoid(M_s+M_r))\n",
    "            H_new = Attention@F.relu(self.W(H_k))\n",
    "            return H_new  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,adjM,input_embeddings):\n",
    "        super(encoder,self).__init__( )\n",
    "        ''' \n",
    "        remember that in pytorch, your input_size is the last dimension of your input\n",
    "        So when my input is F*N, input_size = F\n",
    "        also a row in my matrix corresponds to a cell's representation\n",
    "        '''\n",
    "        ''' For scDEGA, the encoder block remains the same, but the decoder block changes'''\n",
    "        ''' Input embeddings is the embedding vector size for one cell.'''\n",
    "        self.layer1 = graphattention_layer(input_size=input_embeddings\n",
    "                                           ,output_size=512\n",
    "                                           ,adjM=adjM)\n",
    "        self.layer2 = graphattention_layer(input_size=512\n",
    "                                           ,output_size=256\n",
    "                                           ,adjM=adjM)\n",
    "        self.layer3 = graphattention_layer(input_size=256\n",
    "                                           ,output_size=64\n",
    "                                           ,adjM=adjM)\n",
    "    def forward(self, X,A):\n",
    "        '''\n",
    "        X here is the node embeddings, its of shape (N*embedding_size)\n",
    "        I'm gonna tranpose it once in the start, and then at the end.\n",
    "        H3 is of size N*64\n",
    "        '''\n",
    "        H1 = self.layer1(X,A)\n",
    "        H2 = self.layer2(H1,A)\n",
    "        H3 = self.layer3(H2,A)\n",
    "        return H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class gene_decoder(nn.Module):\n",
    "    def __init__(self,adjM,reconstruction_embedding,gene_embeddings):\n",
    "        ''' Reconstruction embedding is the size that your decoded embedding should be.\n",
    "        Simply said, it's the original embedding size for our genes.'''\n",
    "        super(gene_decoder,self).__init__()\n",
    "        self.gene_embeddings = gene_embeddings\n",
    "        gene_embedding_size = gene_embeddings.shape[1] # embedding of a single gene.\n",
    "        ''' Assuming gene_embeddings is 64 * 647,\n",
    "        (647 being the number of genes there) so embedding size is shape[1])'''\n",
    "        self.cell_layer1 = graphattention_layer(input_size=64,\n",
    "                                           output_size=256,adjM=adjM)\n",
    "        self.cell_layer2 = graphattention_layer(input_size=256,\n",
    "                                           output_size=512,adjM=adjM)\n",
    "        self.cell_layer3 = graphattention_layer(input_size=512,\n",
    "                                           output_size=reconstruction_embedding,adjM=adjM)\n",
    "        self.Wcr = nn.Parameter(\n",
    "            th.randn(reconstruction_embedding,reconstruction_embedding+gene_embedding_size))\n",
    "        self.Wgr = nn.Parameter(\n",
    "            th.randn(gene_embedding_size,gene_embedding_size+reconstruction_embedding))\n",
    "    def forward(self, H,A):\n",
    "        '''\n",
    "        H here is the encoder's output\n",
    "        I'm gonna stack the gene embeddings to the H matrix\n",
    "        Encoder should have returned a 64*N matrix\n",
    "        Gene embeddings should be of dimension num_nodes*64 , which was 647 for the first run.\n",
    "        So we're concatenating a 647*64 matrix to a N*64 matrix\n",
    "        '''\n",
    "        # Hpass is a 64*(N+647) matrix\n",
    "        Hpass = th.stack(H,self.gene_embeddings) \n",
    "        # We're passing in the (N+647)*64 matrix \n",
    "        cell_H1 = self.layer1(Hpass,A)\n",
    "        cell_H2 = self.layer2(cell_H1,A)\n",
    "        cell_H3= self.layer3(cell_H2,A)\n",
    "        X_cr = self.Wcr@cell_H3 # cell reconstructed matrix.\n",
    "        X_gr = self.Wgr@Hpass\n",
    "        ''' H3 would be of size N*N'''\n",
    "        return (X_cr,X_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "class clustering_layer(nn.Module):\n",
    "    def __init__(self,cell_embeddings,n_clusters,initial_cluster_assignments=\"k_means\"):\n",
    "        ''' This is the clustering optimization layer for my model.\n",
    "        The input will be my cell embeddings, and the output will be the cluster assignments.\n",
    "        Initial cluster assignments will be based on the kmeans algorithm.\n",
    "        Each step, I'm going to update the cluster assignments based on the cell embeddings.\n",
    "        Basically, I'm going to update the Q matrix, which is the cluster assignments matrix.\n",
    "        '''\n",
    "        super(clustering_layer,self).__init__()\n",
    "        n_cells = cell_embeddings.shape[0]\n",
    "        embedding_size = cell_embeddings.shape[1]\n",
    "        self.n_clusters = n_clusters\n",
    "        if(initial_cluster_assignments == \"k_means\"):\n",
    "            self.clustering_method = KMeans(n_clusters=n_clusters)\n",
    "        self.center_embeddings = nn.Parameter(th.randn(n_clusters,embedding_size))\n",
    "\n",
    "    def calculate_membership_matrix(cell_embeddings, center_embeddings):\n",
    "        ''' \n",
    "        This function calculates the membership matrix for the cell embeddings.\n",
    "        The membership matrix is of size N*C, where N is the number of nodes, and C is the number of clusters.\n",
    "        The membership matrix is calculated using the t-distribution kernel, with dof 1.\n",
    "        '''\n",
    "        \n",
    "        # Calculate the pairwise distances between cell embeddings and center embeddings\n",
    "        # Expand dimensions to enable broadcasting\n",
    "        cell_embeddings_expanded = cell_embeddings.unsqueeze(1)  # Shape: (N, 1, D)\n",
    "        center_embeddings_expanded = center_embeddings.unsqueeze(0)  # Shape: (1, C, D)\n",
    "        \n",
    "        # Compute distances using broadcasting\n",
    "        distances = th.norm(cell_embeddings_expanded - center_embeddings_expanded, dim=2)  # Shape: (N, C)\n",
    "        # Calculate membership values\n",
    "        membership_matrix = 1 / (1 + distances)  # Shape: (N, C)\n",
    "        return membership_matrix\n",
    "\n",
    "    def calculate_auxilliary_matrix(Q):\n",
    "        \"\"\"\n",
    "        Compute matrix P from matrix Q as per the given formula:\n",
    "        P_ij = (q_ij^2 * f_j) / sum_j (q_ij^2 * f_j),\n",
    "        where f_j is the soft cluster frequency for column j of Q.\n",
    "        \n",
    "        Parameters:\n",
    "        Q (torch.Tensor): Input matrix Q of shape (n, m).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output matrix P of shape (n, m).\n",
    "        \"\"\"\n",
    "        Q_squared = Q ** 2\n",
    "        f_j = Q.sum(dim=0)  # Shape (m,)\n",
    "        numerator = Q_squared/f_j\n",
    "        denominator = numerator.sum(dim=1, keepdim=True)  # Shape (n, 1)\n",
    "        P = numerator / denominator\n",
    "        return P\n",
    "\n",
    "    def forward(self, H):\n",
    "        ''' The size of the Q matrix,\n",
    "        will be N*C, where N is the number of nodes, and C is the number of clusters.'''\n",
    "        # With DEC, here the paper scGAC uses a t-distribution kernel of dof 1.\n",
    "        ## I'm going to use the same.\n",
    "        self.clustering_method.fit(H)\n",
    "        center_embeddings = self.clustering_method.cluster_centers_\n",
    "        Q = self.calculate_membership_matrix(H,center_embeddings)\n",
    "        P = self.calculate_auxilliary_matrix(Q)\n",
    "        self.kl_loss = F.kl_div(P,Q)\n",
    "        cluster_assignments = th.argmax(Q,dim=1)\n",
    "        return (cluster_assignments,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scdEGA(nn.Module):\n",
    "    def __init__(self,hidden_size,cellGraph,adjM,GeneGraph):\n",
    "        '''GeneGraph is the gene embeddings from the node2vec model\n",
    "           run on a PPI graph constructed from the gene interactions.\n",
    "           This will be the gene matrix (so gene loss) we wish to reconstruct.\n",
    "                      \n",
    "           Cellmatrix_pca is the PCA reduced cell matrix.\n",
    "           This will be the cell matrix (so cell loss) we wish to reconstruct.\n",
    "\n",
    "           adjM is the adjacency matrix of the cell graph.\n",
    "        '''\n",
    "        super(scdEGA,self).__init__()\n",
    "        self.gc = cellGraph\n",
    "        self.gg = GeneGraph\n",
    "        cell_embeddingsize = cellGraph.shape[1]\n",
    "        gene_embeddingsize = GeneGraph.shape[1]\n",
    "        self.encoder = encoder(adjM,cell_embeddingsize)\n",
    "        self.decoder = gene_decoder(adjM,gene_embeddingsize,GeneGraph)\n",
    "        self.clustering_layer = clustering_layer(hidden_size)\n",
    "    def forward(self, H,A=None):\n",
    "        ## Self-supervised optmization part\n",
    "        cell_embeddings = self.encoder(H,A)\n",
    "        reconstructed_cell_matrix,reconstructed_gene_matrix = self.decoder(cell_embeddings,A)\n",
    "        self.reconstruction_cell_loss = F.cosine_similarity(reconstructed_cell_matrix,self.gc)\n",
    "        self.reconstruction_gene_loss = F.mean_absolute_error(reconstructed_gene_matrix,self.gg)\n",
    "        self.selfsupervised_loss = self.reconstruction_cell_loss+self.reconstruction_gene_loss\n",
    "        ## Clustering optimization part\n",
    "        cluster_assignments,Q = self.clustering_layer(cell_embeddings)\n",
    "        self.kl_loss = self.clustering_layer.kl_loss\n",
    "        self.total_loss = self.selfsupervised_loss+self.kl_loss\n",
    "        return (reconstructed_cell_matrix,reconstructed_gene_matrix,cluster_assignments,Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "def calculate_membership_matrix(cell_embeddings, center_embeddings):\n",
    "    ''' \n",
    "    This function calculates the membership matrix for the cell embeddings.\n",
    "    The membership matrix is of size N*C, where N is the number of nodes, and C is the number of clusters.\n",
    "    The membership matrix is calculated using the t-distribution kernel, with dof 1.\n",
    "    '''\n",
    "    \n",
    "    # Calculate the pairwise distances between cell embeddings and center embeddings\n",
    "    # Expand dimensions to enable broadcasting\n",
    "    cell_embeddings_expanded = cell_embeddings.unsqueeze(1)  # Shape: (N, 1, D)\n",
    "    center_embeddings_expanded = center_embeddings.unsqueeze(0)  # Shape: (1, C, D)\n",
    "    \n",
    "    # Compute distances using broadcasting\n",
    "    distances = th.norm(cell_embeddings_expanded - center_embeddings_expanded, dim=2)  # Shape: (N, C)\n",
    "    \n",
    "    # Calculate membership values\n",
    "    membership_matrix = 1 / (1 + distances)  # Shape: (N, C)\n",
    "\n",
    "    return membership_matrix\n",
    "cell_embeddings = th.randn(10, 64)\n",
    "center_embeddings = th.randn(5, 64)\n",
    "print(calculate_membership_matrix(cell_embeddings, center_embeddings).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellGraphDataset(NodeLoader):\n",
    "    def __init__(self,cell_embeddings,adjM):\n",
    "        self.cell_embeddings = cell_embeddings\n",
    "        self.adjM = adjM\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.cell_embeddings,self.adjM)\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
