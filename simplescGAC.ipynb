{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch_geometric as tg\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import csv\n",
    "from node2vec import Node2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 data_path, \n",
    "                 k=5, \n",
    "                 gene_embedding_dim=64, \n",
    "                 string_file=\"string_interactions.tsv\", \n",
    "                 string_query=False, \n",
    "                 output_hvg=\"highly_variable_genes.txt\"):\n",
    "        \"\"\"\n",
    "        Initialize the data processor with the given parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : str\n",
    "            Name of the dataset (e.g., 'Biase', 'Darmanis', etc.)\n",
    "        data_path : str\n",
    "            Path to the input data file.\n",
    "        k : int\n",
    "            Number of nearest neighbors for adjacency construction.\n",
    "        gene_embedding_dim : int\n",
    "            Dimension of the gene embeddings (e.g., 64).\n",
    "        string_file : str\n",
    "            Path to the STRING interactions file (TSV).\n",
    "        string_query : bool\n",
    "            If True, attempt to query the STRING database for interactions.\n",
    "        output_hvg : str\n",
    "            File to write HVGs.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.data_path = data_path\n",
    "        self.k = k\n",
    "        self.gene_embedding_dim = gene_embedding_dim\n",
    "        self.string_file = string_file\n",
    "        self.string_query = string_query\n",
    "        self.output_hvg = output_hvg\n",
    "\n",
    "    def load_dataset(self, dataset_name, data_path):\n",
    "        \"\"\"\n",
    "        Load the dataset specified by dataset_name.\n",
    "        Implement your own logic here depending on how your datasets are stored.\n",
    "        \"\"\"\n",
    "        # Placeholder: For now assume a text file containing genes x cells\n",
    "        print(f\"this is  {data_path}\")\n",
    "        adata = sc.read_text(data_path)\n",
    "        \n",
    "        return adata\n",
    "\n",
    "    def preprocess_data(self, adata):\n",
    "        \"\"\"\n",
    "        Preprocess the data:\n",
    "        - Transpose if necessary\n",
    "        - PCA\n",
    "        - Filter genes/cells\n",
    "        - Compute QC metrics, etc.\n",
    "        \"\"\"\n",
    "        adata = adata.T\n",
    "        adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "        adata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\",\"RPL\"))\n",
    "        adata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n",
    "\n",
    "        sc.pp.calculate_qc_metrics(\n",
    "            adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20], log1p=True\n",
    "        )\n",
    "\n",
    "        # Basic filtering\n",
    "        sc.pp.filter_cells(adata, min_genes=3)\n",
    "\n",
    "        # PCA and neighbors\n",
    "        sc.pp.pca(adata)\n",
    "        sc.pp.neighbors(adata)\n",
    "\n",
    "        # Normalize and log transform\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "\n",
    "        # Find HVGs\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "        return adata\n",
    "\n",
    "    def build_adjacency(self, adata, k):\n",
    "        \"\"\"\n",
    "        Build an adjacency (KNN) graph for cells.\n",
    "        \"\"\"\n",
    "        distances = adata.obsp[\"distances\"]\n",
    "        distances_csr = distances.tocsr()\n",
    "        N = adata.shape[0]\n",
    "        adj_list = []\n",
    "        for i in range(N):\n",
    "            row_indices = distances_csr[i].indices\n",
    "            row_data = distances_csr[i].data\n",
    "            # Pick top k neighbors\n",
    "            k_neighbors = row_indices[row_data.argsort()[:k]]\n",
    "            adj_list.append(k_neighbors)       \n",
    "        A = th.zeros(size=(N, N))\n",
    "        adj_list = np.array(adj_list)\n",
    "        for i in range(adj_list.shape[0]):\n",
    "            for j in range(k):\n",
    "                A[i, adj_list[i][j]] = 1\n",
    "\n",
    "        edge_list = th.zeros(size=(2, N*k))\n",
    "        for i in range(N):\n",
    "            for j in range(k):\n",
    "                edge_list[0, i*k+j] = i\n",
    "                edge_list[1, i*k+j] = adj_list[i][j]        \n",
    "        return A, adj_list,edge_list\n",
    "\n",
    "    def write_hvg_list(self, adata, output_file):\n",
    "        hvgs = adata.var_names[adata.var[\"highly_variable\"]]\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\",\".join(hvgs))\n",
    "        return hvgs\n",
    "\n",
    "    def query_string_db(self, hvgs, output_file):\n",
    "        \"\"\"\n",
    "        Query the STRING database for interactions among the given list of HVGs.\n",
    "        This is pseudo-code. Implement it based on the STRING API.\n",
    "        \"\"\"\n",
    "        # For now, assume string_interactions.tsv is already available.\n",
    "        # In practice, you'd implement an API call here.\n",
    "        pass\n",
    "\n",
    "    def build_gene_graph_and_embed(self, string_file, emb_dim):\n",
    "        \"\"\"\n",
    "        Build the gene graph from string_interactions.tsv and run node2vec.\n",
    "        \"\"\"\n",
    "        edge_list = pd.DataFrame(csv.reader(open(string_file), delimiter=\"\\t\"))\n",
    "        edge_list.columns = edge_list.iloc[0]\n",
    "        edge_list = edge_list.drop(0)\n",
    "\n",
    "        gene_graph = nx.Graph()\n",
    "        gene_graph.add_edges_from(edge_list[[\"node1_string_id\",\"node2_string_id\"]].values)\n",
    "\n",
    "        node2vec_model = Node2Vec(gene_graph, dimensions=emb_dim, walk_length=30, num_walks=200, workers=4)\n",
    "        model = node2vec_model.fit(window=10, min_count=1, batch_words=4)\n",
    "        node_embeddings = model.wv.vectors\n",
    "        return node_embeddings\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Run the entire pipeline:\n",
    "        - Load dataset\n",
    "        - Preprocess\n",
    "        - Build adjacency\n",
    "        - Write HVG list\n",
    "        - Optionally query STRING\n",
    "        - Build gene embeddings\n",
    "        Return adata, adjacency matrix A, and node_embeddings.\n",
    "        \"\"\"\n",
    "        # 1. Load dataset\n",
    "        adata = self.load_dataset(self.dataset, self.data_path)\n",
    "\n",
    "        # 2. Preprocess\n",
    "        adata = self.preprocess_data(adata)\n",
    "\n",
    "        # 3. Build adjacency\n",
    "        A, adj_list,edge_list = self.build_adjacency(adata, self.k)\n",
    "\n",
    "        # 4. Write HVG list\n",
    "        hvgs = self.write_hvg_list(adata, self.output_hvg)\n",
    "\n",
    "        # 5. Optionally query STRING DB\n",
    "        if self.string_query:\n",
    "            self.query_string_db(hvgs, self.string_file)\n",
    "\n",
    "        # 6. Build gene graph and node embeddings\n",
    "        node_embeddings = self.build_gene_graph_and_embed(self.string_file, self.gene_embedding_dim)\n",
    "        return adata, A, node_embeddings,edge_list\n",
    "\n",
    "\n",
    "processor = DataProcessor(\n",
    "    dataset=\"Biase\",\n",
    "    data_path=\"./tests/datastuff/GSE57249_fpkm.txt\",\n",
    "    k=5,\n",
    "    gene_embedding_dim=64,\n",
    "    string_file=\"./tests/datastuff/string_interactions.tsv\",\n",
    "    string_query=False,  # Set to True if you implement the query function\n",
    "    output_hvg=\"./tests/datastuff/highly_variable_genes.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepEmbeddedClustering(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, n_clusters, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Deep Embedded Clustering Model\n",
    "        \n",
    "        Args:\n",
    "        - input_dim (int): Dimension of input features\n",
    "        - hidden_dims (list): List of hidden layer dimensions\n",
    "        - n_clusters (int): Number of clusters\n",
    "        - alpha (float): Hyperparameter for soft assignment\n",
    "        \"\"\"\n",
    "        super(DeepEmbeddedClustering, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(GATConv(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(GATConv(hidden_dim, prev_dim))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers = decoder_layers[::-1]\n",
    "        # self.encoder = encoder_layers\n",
    "        # self.decoder = decoder_layers\n",
    "        self.encoder = nn.ModuleList(encoder_layers)\n",
    "        self.decoder = nn.ModuleList(decoder_layers)\n",
    "        \n",
    "        # Clustering layer\n",
    "        self.clustering_layer = nn.Linear(hidden_dims[-1], n_clusters, bias=False)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.alpha = alpha\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def encode(self, x, graphedge_list):\n",
    "        \"\"\"Forward pass through the encoder\"\"\"\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, graphedge_list)\n",
    "        return x\n",
    "    def decode(self, x, graphedge_list):\n",
    "        \"\"\"Forward pass through the decoder\"\"\"\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, graphedge_list)\n",
    "        return x \n",
    "    def forward(self, x,graph_edge_list):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        z = self.encode(x,graph_edge_list)\n",
    "        q = self._soft_assignment(z)\n",
    "        return z, q\n",
    "    def _soft_assignment(self, z):\n",
    "        \"\"\"Compute soft assignment probabilities\"\"\"\n",
    "        # Compute similarity between embedded points and cluster centers\n",
    "        weights = self.clustering_layer.weight\n",
    "        q = 1.0 / ((1.0 + th.sum((z.unsqueeze(1) - weights)**2, dim=2) / self.alpha) + 1e-8) ** ((self.alpha + 1.0) / 2.0)\n",
    "        q = q / th.sum(q, dim=1, keepdim=True)\n",
    "        return q\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        \"\"\"\n",
    "        Compute target distribution (sharpened version of q)\n",
    "        \n",
    "        Args:\n",
    "        - q (th.Tensor): Soft assignment probabilities\n",
    "        \n",
    "        Returns:\n",
    "        - p (th.Tensor): Sharpened target distribution\n",
    "        \"\"\"\n",
    "        p = q**2 / th.sum(q, dim=0)\n",
    "        p = p / th.sum(p, dim=1, keepdim=True)\n",
    "        return p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_autoencoder(model, data,adj_list, epochs=50, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Pretrain autoencoder for initial feature extraction\n",
    "    \n",
    "    Args:\n",
    "    - model (DeepEmbeddedClustering): DEC model\n",
    "    - data (th.Tensor): Input data\n",
    "    - epochs (int): Number of pretraining epochs\n",
    "    - lr (float): Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - Pretrained model\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(data, adj_list)\n",
    "        reconstruction = model.decode(z, adj_list)\n",
    "        loss = criterion(reconstruction, data)\n",
    "        \n",
    "        # Add gradient norm logging\n",
    "        total_norm = sum(p.grad.data.norm(2).item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Pretraining Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}, Grad Norm: {total_norm:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dec_loss(p, q):\n",
    "    \"\"\"\n",
    "    Compute DEC loss (KL divergence between p and q)\n",
    "    \n",
    "    Args:\n",
    "    - p (th.Tensor): Target distribution\n",
    "    - q (th.Tensor): Soft assignment probabilities\n",
    "    \n",
    "    Returns:\n",
    "    - loss (th.Tensor): KL divergence loss\n",
    "    \"\"\"\n",
    "    return th.mean(th.sum(p * th.log(p / q), dim=1))\n",
    "\n",
    "def train_dec(model,edge_list, data, epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train Deep Embedded Clustering model\n",
    "    \n",
    "    Args:\n",
    "    - model (DeepEmbeddedClustering): DEC model\n",
    "    - data (th.Tensor): Input data\n",
    "    - epochs (int): Number of training epochs\n",
    "    - lr (float): Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model and cluster assignments\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize cluster centers using KMeans\n",
    "    with th.no_grad():\n",
    "        z = model.encode(data,edge_list)\n",
    "        kmeans = KMeans(n_clusters=model.n_clusters, n_init=20)\n",
    "        cluster_labels = kmeans.fit_predict(z.numpy())\n",
    "        \n",
    "        # Set initial cluster centers\n",
    "        model.clustering_layer.weight.copy_(\n",
    "            th.from_numpy(kmeans.cluster_centers_).float()\n",
    "        )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        z, q = model(data,edge_list)\n",
    "        \n",
    "        # Compute target distribution\n",
    "        p = model.target_distribution(q)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = dec_loss(p, q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'DEC Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Get final cluster assignments\n",
    "    _, q = model(data,edge_list)\n",
    "    cluster_assignments = th.argmax(q, dim=1)\n",
    "    \n",
    "    return model, cluster_assignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is  ./datasets/Biase/GSE57249_fpkm.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\scanpy\\preprocessing\\_highly_variable_genes.py:305: RuntimeWarning: invalid value encountered in log\n",
      "  dispersion = np.log(dispersion)\n",
      "Computing transition probabilities: 100%|██████████| 647/647 [00:00<00:00, 3901.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simulated data\n",
    "processor = DataProcessor(\n",
    "    dataset=\"Biase\",\n",
    "    data_path=\"./datasets/Biase/GSE57249_fpkm.txt\",\n",
    "    k=5,\n",
    "    gene_embedding_dim=64,\n",
    "    string_file=\"./datasets/Biase/string_interactions.tsv\",\n",
    "    string_query=False,  # Set to True if you implement the query function\n",
    "    output_hvg=\"./datasets/Biase/highly_variable_genes.txt\"\n",
    ")\n",
    "adata, A, node_embeddings,edge_list = processor.get_data()\n",
    "# so our input is the sc matrix of only the hvg genes\n",
    "hvg = adata.var['highly_variable']\n",
    "adata_hvg = adata[:, hvg].X\n",
    "data = th.tensor(adata_hvg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data.shape[1]\n",
    "hidden_dims = [256, 128, 64]\n",
    "n_clusters = 3\n",
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Epoch [0/200], Loss: 0.4040, Grad Norm: 0.0000\n",
      "Pretraining Epoch [10/200], Loss: 0.2992, Grad Norm: 0.0000\n",
      "Pretraining Epoch [20/200], Loss: 0.2413, Grad Norm: 0.0000\n",
      "Pretraining Epoch [30/200], Loss: 0.1951, Grad Norm: 0.0000\n",
      "Pretraining Epoch [40/200], Loss: 0.1713, Grad Norm: 0.0000\n",
      "Pretraining Epoch [50/200], Loss: 0.1557, Grad Norm: 0.0000\n",
      "Pretraining Epoch [60/200], Loss: 0.1465, Grad Norm: 0.0000\n",
      "Pretraining Epoch [70/200], Loss: 0.1392, Grad Norm: 0.0000\n",
      "Pretraining Epoch [80/200], Loss: 0.1331, Grad Norm: 0.0000\n",
      "Pretraining Epoch [90/200], Loss: 0.1276, Grad Norm: 0.0000\n",
      "Pretraining Epoch [100/200], Loss: 0.1224, Grad Norm: 0.0000\n",
      "Pretraining Epoch [110/200], Loss: 0.1179, Grad Norm: 0.0000\n",
      "Pretraining Epoch [120/200], Loss: 0.1140, Grad Norm: 0.0000\n",
      "Pretraining Epoch [130/200], Loss: 0.1106, Grad Norm: 0.0000\n",
      "Pretraining Epoch [140/200], Loss: 0.1075, Grad Norm: 0.0000\n",
      "Pretraining Epoch [150/200], Loss: 0.1045, Grad Norm: 0.0000\n",
      "Pretraining Epoch [160/200], Loss: 0.1015, Grad Norm: 0.0000\n",
      "Pretraining Epoch [170/200], Loss: 0.0980, Grad Norm: 0.0000\n",
      "Pretraining Epoch [180/200], Loss: 0.0947, Grad Norm: 0.0000\n",
      "Pretraining Epoch [190/200], Loss: 0.0917, Grad Norm: 0.0000\n",
      "DEC Epoch [0/5000], Loss: 0.0731\n",
      "DEC Epoch [10/5000], Loss: 0.0610\n",
      "DEC Epoch [20/5000], Loss: 0.0445\n",
      "DEC Epoch [30/5000], Loss: 0.0355\n",
      "DEC Epoch [40/5000], Loss: 0.0288\n",
      "DEC Epoch [50/5000], Loss: 0.0209\n",
      "DEC Epoch [60/5000], Loss: 0.0163\n",
      "DEC Epoch [70/5000], Loss: 0.0148\n",
      "DEC Epoch [80/5000], Loss: 0.0141\n",
      "DEC Epoch [90/5000], Loss: 0.0137\n",
      "DEC Epoch [100/5000], Loss: 0.0135\n",
      "DEC Epoch [110/5000], Loss: 0.0133\n",
      "DEC Epoch [120/5000], Loss: 0.0131\n",
      "DEC Epoch [130/5000], Loss: 0.0131\n",
      "DEC Epoch [140/5000], Loss: 0.0129\n",
      "DEC Epoch [150/5000], Loss: 0.0129\n",
      "DEC Epoch [160/5000], Loss: 0.0127\n",
      "DEC Epoch [170/5000], Loss: 0.0129\n",
      "DEC Epoch [180/5000], Loss: 0.0136\n",
      "DEC Epoch [190/5000], Loss: 0.0127\n",
      "DEC Epoch [200/5000], Loss: 0.0125\n",
      "DEC Epoch [210/5000], Loss: 0.0124\n",
      "DEC Epoch [220/5000], Loss: 0.0124\n",
      "DEC Epoch [230/5000], Loss: 0.0124\n",
      "DEC Epoch [240/5000], Loss: 0.0124\n",
      "DEC Epoch [250/5000], Loss: 0.0122\n",
      "DEC Epoch [260/5000], Loss: 0.0125\n",
      "DEC Epoch [270/5000], Loss: 0.0123\n",
      "DEC Epoch [280/5000], Loss: 0.0120\n",
      "DEC Epoch [290/5000], Loss: 0.0120\n",
      "DEC Epoch [300/5000], Loss: 0.0119\n",
      "DEC Epoch [310/5000], Loss: 0.0119\n",
      "DEC Epoch [320/5000], Loss: 0.0119\n",
      "DEC Epoch [330/5000], Loss: 0.0119\n",
      "DEC Epoch [340/5000], Loss: 0.0119\n",
      "DEC Epoch [350/5000], Loss: 0.0117\n",
      "DEC Epoch [360/5000], Loss: 0.0117\n",
      "DEC Epoch [370/5000], Loss: 0.0124\n",
      "DEC Epoch [380/5000], Loss: 0.0117\n",
      "DEC Epoch [390/5000], Loss: 0.0116\n",
      "DEC Epoch [400/5000], Loss: 0.0117\n",
      "DEC Epoch [410/5000], Loss: 0.0114\n",
      "DEC Epoch [420/5000], Loss: 0.0115\n",
      "DEC Epoch [430/5000], Loss: 0.0114\n",
      "DEC Epoch [440/5000], Loss: 0.0120\n",
      "DEC Epoch [450/5000], Loss: 0.0115\n",
      "DEC Epoch [460/5000], Loss: 0.0112\n",
      "DEC Epoch [470/5000], Loss: 0.0112\n",
      "DEC Epoch [480/5000], Loss: 0.0117\n",
      "DEC Epoch [490/5000], Loss: 0.0116\n",
      "DEC Epoch [500/5000], Loss: 0.0112\n",
      "DEC Epoch [510/5000], Loss: 0.0110\n",
      "DEC Epoch [520/5000], Loss: 0.0109\n",
      "DEC Epoch [530/5000], Loss: 0.0109\n",
      "DEC Epoch [540/5000], Loss: 0.0108\n",
      "DEC Epoch [550/5000], Loss: 0.0108\n",
      "DEC Epoch [560/5000], Loss: 0.0108\n",
      "DEC Epoch [570/5000], Loss: 0.0107\n",
      "DEC Epoch [580/5000], Loss: 0.0107\n",
      "DEC Epoch [590/5000], Loss: 0.0108\n",
      "DEC Epoch [600/5000], Loss: 0.0110\n",
      "DEC Epoch [610/5000], Loss: 0.0106\n",
      "DEC Epoch [620/5000], Loss: 0.0106\n",
      "DEC Epoch [630/5000], Loss: 0.0105\n",
      "DEC Epoch [640/5000], Loss: 0.0105\n",
      "DEC Epoch [650/5000], Loss: 0.0104\n",
      "DEC Epoch [660/5000], Loss: 0.0104\n",
      "DEC Epoch [670/5000], Loss: 0.0110\n",
      "DEC Epoch [680/5000], Loss: 0.0106\n",
      "DEC Epoch [690/5000], Loss: 0.0104\n",
      "DEC Epoch [700/5000], Loss: 0.0104\n",
      "DEC Epoch [710/5000], Loss: 0.0103\n",
      "DEC Epoch [720/5000], Loss: 0.0103\n",
      "DEC Epoch [730/5000], Loss: 0.0101\n",
      "DEC Epoch [740/5000], Loss: 0.0101\n",
      "DEC Epoch [750/5000], Loss: 0.0101\n",
      "DEC Epoch [760/5000], Loss: 0.0103\n",
      "DEC Epoch [770/5000], Loss: 0.0100\n",
      "DEC Epoch [780/5000], Loss: 0.0100\n",
      "DEC Epoch [790/5000], Loss: 0.0101\n",
      "DEC Epoch [800/5000], Loss: 0.0099\n",
      "DEC Epoch [810/5000], Loss: 0.0098\n",
      "DEC Epoch [820/5000], Loss: 0.0099\n",
      "DEC Epoch [830/5000], Loss: 0.0101\n",
      "DEC Epoch [840/5000], Loss: 0.0097\n",
      "DEC Epoch [850/5000], Loss: 0.0097\n",
      "DEC Epoch [860/5000], Loss: 0.0097\n",
      "DEC Epoch [870/5000], Loss: 0.0101\n",
      "DEC Epoch [880/5000], Loss: 0.0097\n",
      "DEC Epoch [890/5000], Loss: 0.0096\n",
      "DEC Epoch [900/5000], Loss: 0.0095\n",
      "DEC Epoch [910/5000], Loss: 0.0095\n",
      "DEC Epoch [920/5000], Loss: 0.0094\n",
      "DEC Epoch [930/5000], Loss: 0.0095\n",
      "DEC Epoch [940/5000], Loss: 0.0106\n",
      "DEC Epoch [950/5000], Loss: 0.0095\n",
      "DEC Epoch [960/5000], Loss: 0.0094\n",
      "DEC Epoch [970/5000], Loss: 0.0093\n",
      "DEC Epoch [980/5000], Loss: 0.0093\n",
      "DEC Epoch [990/5000], Loss: 0.0093\n",
      "DEC Epoch [1000/5000], Loss: 0.0096\n",
      "DEC Epoch [1010/5000], Loss: 0.0092\n",
      "DEC Epoch [1020/5000], Loss: 0.0091\n",
      "DEC Epoch [1030/5000], Loss: 0.0091\n",
      "DEC Epoch [1040/5000], Loss: 0.0090\n",
      "DEC Epoch [1050/5000], Loss: 0.0090\n",
      "DEC Epoch [1060/5000], Loss: 0.0091\n",
      "DEC Epoch [1070/5000], Loss: 0.0093\n",
      "DEC Epoch [1080/5000], Loss: 0.0089\n",
      "DEC Epoch [1090/5000], Loss: 0.0089\n",
      "DEC Epoch [1100/5000], Loss: 0.0089\n",
      "DEC Epoch [1110/5000], Loss: 0.0089\n",
      "DEC Epoch [1120/5000], Loss: 0.0091\n",
      "DEC Epoch [1130/5000], Loss: 0.0089\n",
      "DEC Epoch [1140/5000], Loss: 0.0088\n",
      "DEC Epoch [1150/5000], Loss: 0.0088\n",
      "DEC Epoch [1160/5000], Loss: 0.0088\n",
      "DEC Epoch [1170/5000], Loss: 0.0087\n",
      "DEC Epoch [1180/5000], Loss: 0.0087\n",
      "DEC Epoch [1190/5000], Loss: 0.0086\n",
      "DEC Epoch [1200/5000], Loss: 0.0089\n",
      "DEC Epoch [1210/5000], Loss: 0.0086\n",
      "DEC Epoch [1220/5000], Loss: 0.0087\n",
      "DEC Epoch [1230/5000], Loss: 0.0086\n",
      "DEC Epoch [1240/5000], Loss: 0.0085\n",
      "DEC Epoch [1250/5000], Loss: 0.0085\n",
      "DEC Epoch [1260/5000], Loss: 0.0084\n",
      "DEC Epoch [1270/5000], Loss: 0.0084\n",
      "DEC Epoch [1280/5000], Loss: 0.0084\n",
      "DEC Epoch [1290/5000], Loss: 0.0083\n",
      "DEC Epoch [1300/5000], Loss: 0.0083\n",
      "DEC Epoch [1310/5000], Loss: 0.0083\n",
      "DEC Epoch [1320/5000], Loss: 0.0087\n",
      "DEC Epoch [1330/5000], Loss: 0.0085\n",
      "DEC Epoch [1340/5000], Loss: 0.0082\n",
      "DEC Epoch [1350/5000], Loss: 0.0082\n",
      "DEC Epoch [1360/5000], Loss: 0.0082\n",
      "DEC Epoch [1370/5000], Loss: 0.0081\n",
      "DEC Epoch [1380/5000], Loss: 0.0082\n",
      "DEC Epoch [1390/5000], Loss: 0.0082\n",
      "DEC Epoch [1400/5000], Loss: 0.0082\n",
      "DEC Epoch [1410/5000], Loss: 0.0081\n",
      "DEC Epoch [1420/5000], Loss: 0.0080\n",
      "DEC Epoch [1430/5000], Loss: 0.0080\n",
      "DEC Epoch [1440/5000], Loss: 0.0080\n",
      "DEC Epoch [1450/5000], Loss: 0.0080\n",
      "DEC Epoch [1460/5000], Loss: 0.0079\n",
      "DEC Epoch [1470/5000], Loss: 0.0080\n",
      "DEC Epoch [1480/5000], Loss: 0.0081\n",
      "DEC Epoch [1490/5000], Loss: 0.0079\n",
      "DEC Epoch [1500/5000], Loss: 0.0078\n",
      "DEC Epoch [1510/5000], Loss: 0.0078\n",
      "DEC Epoch [1520/5000], Loss: 0.0078\n",
      "DEC Epoch [1530/5000], Loss: 0.0077\n",
      "DEC Epoch [1540/5000], Loss: 0.0077\n",
      "DEC Epoch [1550/5000], Loss: 0.0077\n",
      "DEC Epoch [1560/5000], Loss: 0.0080\n",
      "DEC Epoch [1570/5000], Loss: 0.0078\n",
      "DEC Epoch [1580/5000], Loss: 0.0077\n",
      "DEC Epoch [1590/5000], Loss: 0.0077\n",
      "DEC Epoch [1600/5000], Loss: 0.0078\n",
      "DEC Epoch [1610/5000], Loss: 0.0076\n",
      "DEC Epoch [1620/5000], Loss: 0.0076\n",
      "DEC Epoch [1630/5000], Loss: 0.0075\n",
      "DEC Epoch [1640/5000], Loss: 0.0077\n",
      "DEC Epoch [1650/5000], Loss: 0.0076\n",
      "DEC Epoch [1660/5000], Loss: 0.0075\n",
      "DEC Epoch [1670/5000], Loss: 0.0074\n",
      "DEC Epoch [1680/5000], Loss: 0.0074\n",
      "DEC Epoch [1690/5000], Loss: 0.0074\n",
      "DEC Epoch [1700/5000], Loss: 0.0074\n",
      "DEC Epoch [1710/5000], Loss: 0.0074\n",
      "DEC Epoch [1720/5000], Loss: 0.0074\n",
      "DEC Epoch [1730/5000], Loss: 0.0074\n",
      "DEC Epoch [1740/5000], Loss: 0.0073\n",
      "DEC Epoch [1750/5000], Loss: 0.0073\n",
      "DEC Epoch [1760/5000], Loss: 0.0073\n",
      "DEC Epoch [1770/5000], Loss: 0.0074\n",
      "DEC Epoch [1780/5000], Loss: 0.0072\n",
      "DEC Epoch [1790/5000], Loss: 0.0072\n",
      "DEC Epoch [1800/5000], Loss: 0.0072\n",
      "DEC Epoch [1810/5000], Loss: 0.0072\n",
      "DEC Epoch [1820/5000], Loss: 0.0072\n",
      "DEC Epoch [1830/5000], Loss: 0.0072\n",
      "DEC Epoch [1840/5000], Loss: 0.0071\n",
      "DEC Epoch [1850/5000], Loss: 0.0073\n",
      "DEC Epoch [1860/5000], Loss: 0.0071\n",
      "DEC Epoch [1870/5000], Loss: 0.0070\n",
      "DEC Epoch [1880/5000], Loss: 0.0070\n",
      "DEC Epoch [1890/5000], Loss: 0.0070\n",
      "DEC Epoch [1900/5000], Loss: 0.0073\n",
      "DEC Epoch [1910/5000], Loss: 0.0071\n",
      "DEC Epoch [1920/5000], Loss: 0.0069\n",
      "DEC Epoch [1930/5000], Loss: 0.0069\n",
      "DEC Epoch [1940/5000], Loss: 0.0070\n",
      "DEC Epoch [1950/5000], Loss: 0.0069\n",
      "DEC Epoch [1960/5000], Loss: 0.0068\n",
      "DEC Epoch [1970/5000], Loss: 0.0069\n",
      "DEC Epoch [1980/5000], Loss: 0.0068\n",
      "DEC Epoch [1990/5000], Loss: 0.0068\n",
      "DEC Epoch [2000/5000], Loss: 0.0069\n",
      "DEC Epoch [2010/5000], Loss: 0.0068\n",
      "DEC Epoch [2020/5000], Loss: 0.0068\n",
      "DEC Epoch [2030/5000], Loss: 0.0069\n",
      "DEC Epoch [2040/5000], Loss: 0.0067\n",
      "DEC Epoch [2050/5000], Loss: 0.0067\n",
      "DEC Epoch [2060/5000], Loss: 0.0067\n",
      "DEC Epoch [2070/5000], Loss: 0.0066\n",
      "DEC Epoch [2080/5000], Loss: 0.0067\n",
      "DEC Epoch [2090/5000], Loss: 0.0066\n",
      "DEC Epoch [2100/5000], Loss: 0.0066\n",
      "DEC Epoch [2110/5000], Loss: 0.0067\n",
      "DEC Epoch [2120/5000], Loss: 0.0066\n",
      "DEC Epoch [2130/5000], Loss: 0.0065\n",
      "DEC Epoch [2140/5000], Loss: 0.0065\n",
      "DEC Epoch [2150/5000], Loss: 0.0067\n",
      "DEC Epoch [2160/5000], Loss: 0.0067\n",
      "DEC Epoch [2170/5000], Loss: 0.0065\n",
      "DEC Epoch [2180/5000], Loss: 0.0065\n",
      "DEC Epoch [2190/5000], Loss: 0.0064\n",
      "DEC Epoch [2200/5000], Loss: 0.0065\n",
      "DEC Epoch [2210/5000], Loss: 0.0066\n",
      "DEC Epoch [2220/5000], Loss: 0.0064\n",
      "DEC Epoch [2230/5000], Loss: 0.0064\n",
      "DEC Epoch [2240/5000], Loss: 0.0064\n",
      "DEC Epoch [2250/5000], Loss: 0.0063\n",
      "DEC Epoch [2260/5000], Loss: 0.0063\n",
      "DEC Epoch [2270/5000], Loss: 0.0064\n",
      "DEC Epoch [2280/5000], Loss: 0.0064\n",
      "DEC Epoch [2290/5000], Loss: 0.0063\n",
      "DEC Epoch [2300/5000], Loss: 0.0063\n",
      "DEC Epoch [2310/5000], Loss: 0.0064\n",
      "DEC Epoch [2320/5000], Loss: 0.0062\n",
      "DEC Epoch [2330/5000], Loss: 0.0062\n",
      "DEC Epoch [2340/5000], Loss: 0.0063\n",
      "DEC Epoch [2350/5000], Loss: 0.0062\n",
      "DEC Epoch [2360/5000], Loss: 0.0061\n",
      "DEC Epoch [2370/5000], Loss: 0.0061\n",
      "DEC Epoch [2380/5000], Loss: 0.0062\n",
      "DEC Epoch [2390/5000], Loss: 0.0061\n",
      "DEC Epoch [2400/5000], Loss: 0.0061\n",
      "DEC Epoch [2410/5000], Loss: 0.0061\n",
      "DEC Epoch [2420/5000], Loss: 0.0060\n",
      "DEC Epoch [2430/5000], Loss: 0.0060\n",
      "DEC Epoch [2440/5000], Loss: 0.0061\n",
      "DEC Epoch [2450/5000], Loss: 0.0060\n",
      "DEC Epoch [2460/5000], Loss: 0.0060\n",
      "DEC Epoch [2470/5000], Loss: 0.0060\n",
      "DEC Epoch [2480/5000], Loss: 0.0060\n",
      "DEC Epoch [2490/5000], Loss: 0.0060\n",
      "DEC Epoch [2500/5000], Loss: 0.0061\n",
      "DEC Epoch [2510/5000], Loss: 0.0060\n",
      "DEC Epoch [2520/5000], Loss: 0.0061\n",
      "DEC Epoch [2530/5000], Loss: 0.0059\n",
      "DEC Epoch [2540/5000], Loss: 0.0059\n",
      "DEC Epoch [2550/5000], Loss: 0.0059\n",
      "DEC Epoch [2560/5000], Loss: 0.0058\n",
      "DEC Epoch [2570/5000], Loss: 0.0058\n",
      "DEC Epoch [2580/5000], Loss: 0.0059\n",
      "DEC Epoch [2590/5000], Loss: 0.0059\n",
      "DEC Epoch [2600/5000], Loss: 0.0058\n",
      "DEC Epoch [2610/5000], Loss: 0.0058\n",
      "DEC Epoch [2620/5000], Loss: 0.0058\n",
      "DEC Epoch [2630/5000], Loss: 0.0057\n",
      "DEC Epoch [2640/5000], Loss: 0.0057\n",
      "DEC Epoch [2650/5000], Loss: 0.0057\n",
      "DEC Epoch [2660/5000], Loss: 0.0058\n",
      "DEC Epoch [2670/5000], Loss: 0.0057\n",
      "DEC Epoch [2680/5000], Loss: 0.0059\n",
      "DEC Epoch [2690/5000], Loss: 0.0057\n",
      "DEC Epoch [2700/5000], Loss: 0.0057\n",
      "DEC Epoch [2710/5000], Loss: 0.0056\n",
      "DEC Epoch [2720/5000], Loss: 0.0057\n",
      "DEC Epoch [2730/5000], Loss: 0.0056\n",
      "DEC Epoch [2740/5000], Loss: 0.0056\n",
      "DEC Epoch [2750/5000], Loss: 0.0057\n",
      "DEC Epoch [2760/5000], Loss: 0.0056\n",
      "DEC Epoch [2770/5000], Loss: 0.0055\n",
      "DEC Epoch [2780/5000], Loss: 0.0056\n",
      "DEC Epoch [2790/5000], Loss: 0.0055\n",
      "DEC Epoch [2800/5000], Loss: 0.0055\n",
      "DEC Epoch [2810/5000], Loss: 0.0056\n",
      "DEC Epoch [2820/5000], Loss: 0.0055\n",
      "DEC Epoch [2830/5000], Loss: 0.0056\n",
      "DEC Epoch [2840/5000], Loss: 0.0055\n",
      "DEC Epoch [2850/5000], Loss: 0.0055\n",
      "DEC Epoch [2860/5000], Loss: 0.0054\n",
      "DEC Epoch [2870/5000], Loss: 0.0054\n",
      "DEC Epoch [2880/5000], Loss: 0.0056\n",
      "DEC Epoch [2890/5000], Loss: 0.0054\n",
      "DEC Epoch [2900/5000], Loss: 0.0054\n",
      "DEC Epoch [2910/5000], Loss: 0.0055\n",
      "DEC Epoch [2920/5000], Loss: 0.0054\n",
      "DEC Epoch [2930/5000], Loss: 0.0053\n",
      "DEC Epoch [2940/5000], Loss: 0.0054\n",
      "DEC Epoch [2950/5000], Loss: 0.0054\n",
      "DEC Epoch [2960/5000], Loss: 0.0054\n",
      "DEC Epoch [2970/5000], Loss: 0.0053\n",
      "DEC Epoch [2980/5000], Loss: 0.0054\n",
      "DEC Epoch [2990/5000], Loss: 0.0053\n",
      "DEC Epoch [3000/5000], Loss: 0.0053\n",
      "DEC Epoch [3010/5000], Loss: 0.0053\n",
      "DEC Epoch [3020/5000], Loss: 0.0053\n",
      "DEC Epoch [3030/5000], Loss: 0.0052\n",
      "DEC Epoch [3040/5000], Loss: 0.0052\n",
      "DEC Epoch [3050/5000], Loss: 0.0052\n",
      "DEC Epoch [3060/5000], Loss: 0.0052\n",
      "DEC Epoch [3070/5000], Loss: 0.0052\n",
      "DEC Epoch [3080/5000], Loss: 0.0052\n",
      "DEC Epoch [3090/5000], Loss: 0.0052\n",
      "DEC Epoch [3100/5000], Loss: 0.0051\n",
      "DEC Epoch [3110/5000], Loss: 0.0052\n",
      "DEC Epoch [3120/5000], Loss: 0.0051\n",
      "DEC Epoch [3130/5000], Loss: 0.0051\n",
      "DEC Epoch [3140/5000], Loss: 0.0051\n",
      "DEC Epoch [3150/5000], Loss: 0.0051\n",
      "DEC Epoch [3160/5000], Loss: 0.0051\n",
      "DEC Epoch [3170/5000], Loss: 0.0051\n",
      "DEC Epoch [3180/5000], Loss: 0.0051\n",
      "DEC Epoch [3190/5000], Loss: 0.0050\n",
      "DEC Epoch [3200/5000], Loss: 0.0050\n",
      "DEC Epoch [3210/5000], Loss: 0.0050\n",
      "DEC Epoch [3220/5000], Loss: 0.0050\n",
      "DEC Epoch [3230/5000], Loss: 0.0050\n",
      "DEC Epoch [3240/5000], Loss: 0.0050\n",
      "DEC Epoch [3250/5000], Loss: 0.0050\n",
      "DEC Epoch [3260/5000], Loss: 0.0050\n",
      "DEC Epoch [3270/5000], Loss: 0.0051\n",
      "DEC Epoch [3280/5000], Loss: 0.0050\n",
      "DEC Epoch [3290/5000], Loss: 0.0049\n",
      "DEC Epoch [3300/5000], Loss: 0.0049\n",
      "DEC Epoch [3310/5000], Loss: 0.0049\n",
      "DEC Epoch [3320/5000], Loss: 0.0049\n",
      "DEC Epoch [3330/5000], Loss: 0.0049\n",
      "DEC Epoch [3340/5000], Loss: 0.0050\n",
      "DEC Epoch [3350/5000], Loss: 0.0048\n",
      "DEC Epoch [3360/5000], Loss: 0.0048\n",
      "DEC Epoch [3370/5000], Loss: 0.0049\n",
      "DEC Epoch [3380/5000], Loss: 0.0049\n",
      "DEC Epoch [3390/5000], Loss: 0.0048\n",
      "DEC Epoch [3400/5000], Loss: 0.0048\n",
      "DEC Epoch [3410/5000], Loss: 0.0049\n",
      "DEC Epoch [3420/5000], Loss: 0.0048\n",
      "DEC Epoch [3430/5000], Loss: 0.0048\n",
      "DEC Epoch [3440/5000], Loss: 0.0048\n",
      "DEC Epoch [3450/5000], Loss: 0.0047\n",
      "DEC Epoch [3460/5000], Loss: 0.0047\n",
      "DEC Epoch [3470/5000], Loss: 0.0048\n",
      "DEC Epoch [3480/5000], Loss: 0.0047\n",
      "DEC Epoch [3490/5000], Loss: 0.0047\n",
      "DEC Epoch [3500/5000], Loss: 0.0047\n",
      "DEC Epoch [3510/5000], Loss: 0.0047\n",
      "DEC Epoch [3520/5000], Loss: 0.0047\n",
      "DEC Epoch [3530/5000], Loss: 0.0051\n",
      "DEC Epoch [3540/5000], Loss: 0.0047\n",
      "DEC Epoch [3550/5000], Loss: 0.0047\n",
      "DEC Epoch [3560/5000], Loss: 0.0046\n",
      "DEC Epoch [3570/5000], Loss: 0.0046\n",
      "DEC Epoch [3580/5000], Loss: 0.0046\n",
      "DEC Epoch [3590/5000], Loss: 0.0047\n",
      "DEC Epoch [3600/5000], Loss: 0.0046\n",
      "DEC Epoch [3610/5000], Loss: 0.0046\n",
      "DEC Epoch [3620/5000], Loss: 0.0046\n",
      "DEC Epoch [3630/5000], Loss: 0.0046\n",
      "DEC Epoch [3640/5000], Loss: 0.0045\n",
      "DEC Epoch [3650/5000], Loss: 0.0045\n",
      "DEC Epoch [3660/5000], Loss: 0.0046\n",
      "DEC Epoch [3670/5000], Loss: 0.0045\n",
      "DEC Epoch [3680/5000], Loss: 0.0045\n",
      "DEC Epoch [3690/5000], Loss: 0.0046\n",
      "DEC Epoch [3700/5000], Loss: 0.0045\n",
      "DEC Epoch [3710/5000], Loss: 0.0045\n",
      "DEC Epoch [3720/5000], Loss: 0.0045\n",
      "DEC Epoch [3730/5000], Loss: 0.0046\n",
      "DEC Epoch [3740/5000], Loss: 0.0045\n",
      "DEC Epoch [3750/5000], Loss: 0.0044\n",
      "DEC Epoch [3760/5000], Loss: 0.0045\n",
      "DEC Epoch [3770/5000], Loss: 0.0045\n",
      "DEC Epoch [3780/5000], Loss: 0.0044\n",
      "DEC Epoch [3790/5000], Loss: 0.0044\n",
      "DEC Epoch [3800/5000], Loss: 0.0044\n",
      "DEC Epoch [3810/5000], Loss: 0.0044\n",
      "DEC Epoch [3820/5000], Loss: 0.0045\n",
      "DEC Epoch [3830/5000], Loss: 0.0044\n",
      "DEC Epoch [3840/5000], Loss: 0.0044\n",
      "DEC Epoch [3850/5000], Loss: 0.0044\n",
      "DEC Epoch [3860/5000], Loss: 0.0044\n",
      "DEC Epoch [3870/5000], Loss: 0.0043\n",
      "DEC Epoch [3880/5000], Loss: 0.0043\n",
      "DEC Epoch [3890/5000], Loss: 0.0044\n",
      "DEC Epoch [3900/5000], Loss: 0.0043\n",
      "DEC Epoch [3910/5000], Loss: 0.0043\n",
      "DEC Epoch [3920/5000], Loss: 0.0043\n",
      "DEC Epoch [3930/5000], Loss: 0.0043\n",
      "DEC Epoch [3940/5000], Loss: 0.0043\n",
      "DEC Epoch [3950/5000], Loss: 0.0043\n",
      "DEC Epoch [3960/5000], Loss: 0.0043\n",
      "DEC Epoch [3970/5000], Loss: 0.0043\n",
      "DEC Epoch [3980/5000], Loss: 0.0043\n",
      "DEC Epoch [3990/5000], Loss: 0.0042\n",
      "DEC Epoch [4000/5000], Loss: 0.0042\n",
      "DEC Epoch [4010/5000], Loss: 0.0043\n",
      "DEC Epoch [4020/5000], Loss: 0.0042\n",
      "DEC Epoch [4030/5000], Loss: 0.0042\n",
      "DEC Epoch [4040/5000], Loss: 0.0042\n",
      "DEC Epoch [4050/5000], Loss: 0.0042\n",
      "DEC Epoch [4060/5000], Loss: 0.0042\n",
      "DEC Epoch [4070/5000], Loss: 0.0041\n",
      "DEC Epoch [4080/5000], Loss: 0.0041\n",
      "DEC Epoch [4090/5000], Loss: 0.0043\n",
      "DEC Epoch [4100/5000], Loss: 0.0042\n",
      "DEC Epoch [4110/5000], Loss: 0.0041\n",
      "DEC Epoch [4120/5000], Loss: 0.0042\n",
      "DEC Epoch [4130/5000], Loss: 0.0041\n",
      "DEC Epoch [4140/5000], Loss: 0.0041\n",
      "DEC Epoch [4150/5000], Loss: 0.0041\n",
      "DEC Epoch [4160/5000], Loss: 0.0041\n",
      "DEC Epoch [4170/5000], Loss: 0.0041\n",
      "DEC Epoch [4180/5000], Loss: 0.0041\n",
      "DEC Epoch [4190/5000], Loss: 0.0040\n",
      "DEC Epoch [4200/5000], Loss: 0.0040\n",
      "DEC Epoch [4210/5000], Loss: 0.0040\n",
      "DEC Epoch [4220/5000], Loss: 0.0040\n",
      "DEC Epoch [4230/5000], Loss: 0.0040\n",
      "DEC Epoch [4240/5000], Loss: 0.0041\n",
      "DEC Epoch [4250/5000], Loss: 0.0040\n",
      "DEC Epoch [4260/5000], Loss: 0.0041\n",
      "DEC Epoch [4270/5000], Loss: 0.0041\n",
      "DEC Epoch [4280/5000], Loss: 0.0040\n",
      "DEC Epoch [4290/5000], Loss: 0.0040\n",
      "DEC Epoch [4300/5000], Loss: 0.0040\n",
      "DEC Epoch [4310/5000], Loss: 0.0040\n",
      "DEC Epoch [4320/5000], Loss: 0.0040\n",
      "DEC Epoch [4330/5000], Loss: 0.0039\n",
      "DEC Epoch [4340/5000], Loss: 0.0039\n",
      "DEC Epoch [4350/5000], Loss: 0.0039\n",
      "DEC Epoch [4360/5000], Loss: 0.0039\n",
      "DEC Epoch [4370/5000], Loss: 0.0039\n",
      "DEC Epoch [4380/5000], Loss: 0.0039\n",
      "DEC Epoch [4390/5000], Loss: 0.0039\n",
      "DEC Epoch [4400/5000], Loss: 0.0039\n",
      "DEC Epoch [4410/5000], Loss: 0.0039\n",
      "DEC Epoch [4420/5000], Loss: 0.0040\n",
      "DEC Epoch [4430/5000], Loss: 0.0039\n",
      "DEC Epoch [4440/5000], Loss: 0.0039\n",
      "DEC Epoch [4450/5000], Loss: 0.0039\n",
      "DEC Epoch [4460/5000], Loss: 0.0038\n",
      "DEC Epoch [4470/5000], Loss: 0.0038\n",
      "DEC Epoch [4480/5000], Loss: 0.0038\n",
      "DEC Epoch [4490/5000], Loss: 0.0038\n",
      "DEC Epoch [4500/5000], Loss: 0.0038\n",
      "DEC Epoch [4510/5000], Loss: 0.0038\n",
      "DEC Epoch [4520/5000], Loss: 0.0038\n",
      "DEC Epoch [4530/5000], Loss: 0.0038\n",
      "DEC Epoch [4540/5000], Loss: 0.0038\n",
      "DEC Epoch [4550/5000], Loss: 0.0038\n",
      "DEC Epoch [4560/5000], Loss: 0.0038\n",
      "DEC Epoch [4570/5000], Loss: 0.0038\n",
      "DEC Epoch [4580/5000], Loss: 0.0037\n",
      "DEC Epoch [4590/5000], Loss: 0.0038\n",
      "DEC Epoch [4600/5000], Loss: 0.0038\n",
      "DEC Epoch [4610/5000], Loss: 0.0037\n",
      "DEC Epoch [4620/5000], Loss: 0.0037\n",
      "DEC Epoch [4630/5000], Loss: 0.0039\n",
      "DEC Epoch [4640/5000], Loss: 0.0037\n",
      "DEC Epoch [4650/5000], Loss: 0.0037\n",
      "DEC Epoch [4660/5000], Loss: 0.0037\n",
      "DEC Epoch [4670/5000], Loss: 0.0037\n",
      "DEC Epoch [4680/5000], Loss: 0.0037\n",
      "DEC Epoch [4690/5000], Loss: 0.0037\n",
      "DEC Epoch [4700/5000], Loss: 0.0037\n",
      "DEC Epoch [4710/5000], Loss: 0.0037\n",
      "DEC Epoch [4720/5000], Loss: 0.0037\n",
      "DEC Epoch [4730/5000], Loss: 0.0036\n",
      "DEC Epoch [4740/5000], Loss: 0.0036\n",
      "DEC Epoch [4750/5000], Loss: 0.0037\n",
      "DEC Epoch [4760/5000], Loss: 0.0037\n",
      "DEC Epoch [4770/5000], Loss: 0.0037\n",
      "DEC Epoch [4780/5000], Loss: 0.0036\n",
      "DEC Epoch [4790/5000], Loss: 0.0037\n",
      "DEC Epoch [4800/5000], Loss: 0.0036\n",
      "DEC Epoch [4810/5000], Loss: 0.0036\n",
      "DEC Epoch [4820/5000], Loss: 0.0036\n",
      "DEC Epoch [4830/5000], Loss: 0.0036\n",
      "DEC Epoch [4840/5000], Loss: 0.0036\n",
      "DEC Epoch [4850/5000], Loss: 0.0036\n",
      "DEC Epoch [4860/5000], Loss: 0.0035\n",
      "DEC Epoch [4870/5000], Loss: 0.0036\n",
      "DEC Epoch [4880/5000], Loss: 0.0035\n",
      "DEC Epoch [4890/5000], Loss: 0.0035\n",
      "DEC Epoch [4900/5000], Loss: 0.0035\n",
      "DEC Epoch [4910/5000], Loss: 0.0035\n",
      "DEC Epoch [4920/5000], Loss: 0.0035\n",
      "DEC Epoch [4930/5000], Loss: 0.0035\n",
      "DEC Epoch [4940/5000], Loss: 0.0035\n",
      "DEC Epoch [4950/5000], Loss: 0.0036\n",
      "DEC Epoch [4960/5000], Loss: 0.0035\n",
      "DEC Epoch [4970/5000], Loss: 0.0036\n",
      "DEC Epoch [4980/5000], Loss: 0.0035\n",
      "DEC Epoch [4990/5000], Loss: 0.0035\n",
      "Clustering complete. Cluster labels: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 2, 0, 2, 0, 2, 2, 2])\n",
      "Cluster labels from DEC model: [2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 2 0 2 0 2 2 2]\n",
      "Cluster labels from sklearn KMeans: [2 2 2 2 2 2 0 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model = DeepEmbeddedClustering(input_dim, hidden_dims, n_clusters)\n",
    "edge_list = edge_list.int()\n",
    "pretrain_autoencoder(model,data,edge_list, epochs=200, lr=2e-4)\n",
    "\n",
    "# Train model\n",
    "trained_model, cluster_labels = train_dec(model,edge_list ,data,epochs=5000,lr=5e-4)\n",
    "\n",
    "print(\"Clustering complete. Cluster labels:\", cluster_labels)\n",
    "\n",
    "# Compare with sklearn clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.numpy())\n",
    "\n",
    "# Fit KMeans from sklearn\n",
    "kmeans_sklearn = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "cluster_labels_sklearn = kmeans_sklearn.fit_predict(data_scaled)\n",
    "\n",
    "# Print comparison\n",
    "print(\"Cluster labels from DEC model:\", cluster_labels.numpy())\n",
    "print(\"Cluster labels from sklearn KMeans:\", cluster_labels_sklearn)\n",
    "\n",
    "# Compute ARI and NMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepEmbeddedClustering(input_dim, hidden_dims, n_clusters)\n",
    "# def debug_gradients(model):\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.grad is None:\n",
    "#             print(f\"No gradient for: {name}\")\n",
    "#         print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient for: clustering_layer.weight\n",
      "clustering_layer.weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# debug_gradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DeepEmbeddedClustering(\n",
       "  (clustering_layer): Linear(in_features=64, out_features=3, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient for: clustering_layer.weight\n",
      "clustering_layer.weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# model.zero_grad()\n",
    "# z = model.encode(data, edge_list)\n",
    "# reconstruction = model.decode(z, edge_list)\n",
    "# loss = nn.MSELoss()(reconstruction, data)\n",
    "# loss.backward()\n",
    "# debug_gradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes = pd.read_csv(\"./datasets/Biase/subtype.ann\",delimiter=\"\\t\",header=None)\n",
    "celltypes.columns = [\"cell\",\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes = pd.read_csv(\"./datasets/Biase/subtype.ann\",delimiter=\"\\t\",header=None)\n",
    "celltypes.columns = [\"cell\",\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['label', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "       '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltypes[\"type\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell cell not found in adata.obs_names\n",
      "Index of cell GSM1377859: 0\n",
      "Index of cell GSM1377860: 1\n",
      "Index of cell GSM1377861: 2\n",
      "Index of cell GSM1377862: 3\n",
      "Index of cell GSM1377863: 4\n",
      "Index of cell GSM1377864: 5\n",
      "Index of cell GSM1377865: 6\n",
      "Index of cell GSM1377866: 7\n",
      "Index of cell GSM1377867: 8\n",
      "Index of cell GSM1377868: 9\n",
      "Index of cell GSM1377869: 10\n",
      "Index of cell GSM1377870: 11\n",
      "Index of cell GSM1377871: 12\n",
      "Index of cell GSM1377872: 13\n",
      "Index of cell GSM1377873: 14\n",
      "Index of cell GSM1377874: 15\n",
      "Index of cell GSM1377875: 16\n",
      "Index of cell GSM1377876: 17\n",
      "Index of cell GSM1377877: 18\n",
      "Index of cell GSM1377878: 19\n",
      "Index of cell GSM1377879: 20\n",
      "Index of cell GSM1377880: 21\n",
      "Index of cell GSM1377881: 22\n",
      "Index of cell GSM1377882: 23\n",
      "Index of cell GSM1377883: 24\n",
      "Index of cell GSM1377884: 25\n",
      "Index of cell GSM1377885: 26\n",
      "Index of cell GSM1377886: 27\n",
      "Index of cell GSM1377887: 28\n",
      "Index of cell GSM1377888: 29\n",
      "Index of cell GSM1377889: 30\n",
      "Index of cell GSM1377890: 31\n",
      "Index of cell GSM1377891: 32\n",
      "Index of cell GSM1377892: 33\n",
      "Index of cell GSM1377893: 34\n",
      "Index of cell GSM1377894: 35\n",
      "Index of cell GSM1377895: 36\n",
      "Index of cell GSM1377896: 37\n",
      "Index of cell GSM1377897: 38\n",
      "Index of cell GSM1377898: 39\n",
      "Index of cell GSM1377899: 40\n",
      "Index of cell GSM1377900: 41\n",
      "Index of cell GSM1377901: 42\n",
      "Index of cell GSM1377902: 43\n",
      "Index of cell GSM1377903: 44\n",
      "Index of cell GSM1377904: 45\n",
      "Index of cell GSM1377905: 46\n",
      "Index of cell GSM1377906: 47\n",
      "Index of cell GSM1377907: 48\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for cell in celltypes[\"cell\"]:\n",
    "    if cell in adata.obs_names:\n",
    "        index = adata.obs_names.get_loc(cell)\n",
    "        print(f\"Index of cell {cell}: {index}\")\n",
    "        indices.append(index)\n",
    "    else:\n",
    "        print(f\"Cell {cell} not found in adata.obs_names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels[:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "       '2', '2', '2', '2', '2', '2', '2', '2', '2', '2'], dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltypes.values[:,1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.9275\n",
      "Normalized Mutual Information (NMI): 0.9218\n",
      "Sklearn ARI and NMI:\n",
      "Adjusted Rand Index (ARI): 0.3746\n",
      "Normalized Mutual Information (NMI): 0.6023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ari = adjusted_rand_score(cluster_labels[:49],celltypes.values[:,1][1:])\n",
    "nmi = normalized_mutual_info_score(cluster_labels[:49],celltypes.values[:,1][1:])\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "\n",
    "print(\"Sklearn ARI and NMI:\")\n",
    "ari = adjusted_rand_score(cluster_labels_sklearn[:49],celltypes.values[:,1][1:])\n",
    "nmi = normalized_mutual_info_score(cluster_labels_sklearn[:49],celltypes.values[:,1][1:])\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
